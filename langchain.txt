import os
import asyncio
from operator import itemgetter
from typing import List, Optional, Any, Dict

# --- FastAPI Imports ---
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RAG Chatbot UI</title>
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- React and ReactDOM CDNs -->
    <script crossorigin src="https://unpkg.com/react@18/umd/react.production.min.js"></script>
    <script crossorigin src="https://unpkg.com/react-dom@18/umd/react-dom.production.min.js"></script>
    <!-- Babel CDN for JSX transformation in the browser -->
    <script crossorigin src="https://unpkg.com/@babel/standalone/babel.min.js"></script>

    <!-- Custom CSS for Inter font and animations -->
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f9fafb; /* Light gray background */
        }
        /* Custom animations for typing indicator */
        @keyframes pulse-dot {
            0%, 100% { opacity: 0.3; }
            50% { opacity: 1; }
        }
        .animate-pulse-dot {
            animation: pulse-dot 1.5s infinite;
        }
        .animate-pulse-dot-delay {
            animation: pulse-dot 1.5s infinite 0.2s;
        }
        .animate-pulse-dot-delay-more {
            animation: pulse-dot 1.5s infinite 0.4s;
        }
        /* Ensure textarea auto-resizes correctly */
        textarea {
            box-sizing: border-box; /* Include padding and border in the element's total width and height */
        }
    </style>
</head>
<body>
    <div id="root"></div>

    <!-- Your React App component embedded here -->
    <script type="text/babel">
        // Main App component
        const App = () => {
            const [messages, setMessages] = React.useState([]);
            const [input, setInput] = React.useState('');
            const [isTyping, setIsTyping] = React.useState(false);
            const messagesEndRef = React.useRef(null);
            const textareaRef = React.useRef(null); // Ref for textarea to auto-resize

            // Scroll to the bottom of the chat when new messages arrive
            const scrollToBottom = () => {
                messagesEndRef.current?.scrollIntoView({ behavior: "smooth" });
            };

            // Auto-resize textarea based on content
            const adjustTextareaHeight = () => {
                if (textareaRef.current) {
                    textareaRef.current.style.height = 'auto'; // Reset height
                    textareaRef.current.style.height = `${textareaRef.current.scrollHeight}px`; // Set to scroll height
                }
            };

            React.useEffect(() => {
                scrollToBottom();
            }, [messages]);

            React.useEffect(() => {
                adjustTextareaHeight();
            }, [input]); // Adjust height whenever input changes

            // Handle sending messages to the backend
            const handleSendMessage = async () => {
                if (input.trim() === '') return;

                const userMessage = { id: Date.now(), text: input, sender: 'user' };
                setMessages((prevMessages) => [...prevMessages, userMessage]);
                setInput(''); // Clear input immediately
                setIsTyping(true);

                // --- Backend Integration ---
                try {
                    // *** IMPORTANT: This is the updated fetch URL for Kubeflow Jupyter proxy ***
                    // It assumes your FastAPI backend is running on port 8080 inside the Jupyter container.
                    // If this path doesn't work, you might need to consult your specific Kubeflow proxy setup.
                    const response = await fetch('/user-redirect/proxy/8080/chat', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json',
                        },
                        // Send the user's message and the current chat history to the backend
                        body: JSON.stringify({ message: userMessage.text, chat_history: messages }),
                    });

                    // Check if the HTTP response was successful
                    if (!response.ok) {
                        const errorData = await response.json();
                        throw new Error(`HTTP error! Status: ${response.status}, Detail: ${errorData.detail || 'Unknown error'}`);
                    }

                    const data = await response.json();
                    const botResponse = data.response; // Assuming your backend returns { "response": "..." }
                    setMessages((prevMessages) => [...prevMessages, { id: Date.now() + 1, text: botResponse, sender: 'bot' }]);
                } catch (error) {
                    console.error('Error communicating with backend:', error);
                    setMessages((prevMessages) => [...prevMessages, { id: Date.now() + 1, text: `Oops! Could not connect to the AI. Error: ${error.message}` }]);
                } finally {
                    setIsTyping(false);
                }

                // --- Simulated Bot Response (COMMENTED OUT) ---
                // This block is now commented out as we are connecting to the backend.
                /*
                setTimeout(() => {
                    const botResponse = `I received your message: "${userMessage.text}". I am a RAG chatbot with memory. How can I assist you further today?`;
                    setMessages((prevMessages) => [...prevMessages, { id: Date.now() + 1, text: botResponse, sender: 'bot' }]);
                    setIsTyping(false);
                }, 1500);
                */
            };

            const handleKeyPress = (e) => {
                if (e.key === 'Enter' && !e.shiftKey) { // Send on Enter, allow Shift+Enter for new line
                    e.preventDefault();
                    handleSendMessage();
                }
            };

            return (
                <div className="flex items-center justify-center min-h-screen bg-gray-50 p-4 font-inter text-gray-800">
                    <div className="flex flex-col w-full max-w-3xl h-[90vh] bg-white rounded-xl shadow-2xl overflow-hidden border border-gray-200">
                        {/* Chat Header */}
                        <div className="p-4 bg-gray-50 border-b border-gray-200 flex items-center justify-between">
                            <h1 className="text-xl font-semibold text-gray-700">Your AI Assistant</h1>
                            {/* Optional: Add a subtle status indicator */}
                            <span className="text-xs text-gray-500">Online</span>
                        </div>

                        {/* Chat Messages Area */}
                        <div className="flex-1 p-6 overflow-y-auto space-y-4 bg-white">
                            {messages.length === 0 && (
                                <div className="flex flex-col items-center justify-center h-full text-gray-400">
                                    <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" strokeWidth="1.5" stroke="currentColor" className="w-12 h-12 mb-4">
                                        <path strokeLinecap="round" strokeLinejoin="round" d="M8.625 12a.375.375 0 11-.75 0 .375.375 0 01.75 0zm0 0H8.25m4.125 0a.375.375 0 11-.75 0 .375.375 0 01.75 0zm0 0h.375m4.125 0a.375.375 0 11-.75 0 .375.375 0 01.75 0zm0 0h.375" />
                                    </svg>
                                    <p className="text-lg">Start a new conversation</p>
                                    <p className="text-sm mt-1">Ask me anything!</p>
                                </div>
                            )}
                            {messages.map((msg) => (
                                <div
                                    key={msg.id}
                                    className={`flex ${
                                        msg.sender === 'user' ? 'justify-end' : 'justify-start'
                                    }`}
                                >
                                    <div
                                        className={`max-w-[80%] p-3 rounded-xl shadow-sm leading-relaxed text-sm ${
                                            msg.sender === 'user'
                                                ? 'bg-blue-500 text-white rounded-br-none'
                                                : 'bg-gray-100 text-gray-800 rounded-bl-none'
                                        }`}
                                        style={{
                                            boxShadow: msg.sender === 'user' ? '0 2px 4px rgba(0,0,0,0.1)' : '0 2px 4px rgba(0,0,0,0.05)',
                                            border: msg.sender === 'bot' ? '1px solid rgba(0,0,0,0.05)' : 'none'
                                        }}
                                    >
                                        {msg.text}
                                    </div>
                                </div>
                            ))}
                            {isTyping && (
                                <div className="flex justify-start">
                                    <div className="max-w-[80%] p-3 rounded-xl shadow-sm bg-gray-100 text-gray-800 rounded-bl-none"
                                        style={{ border: '1px solid rgba(0,0,0,0.05)' }}>
                                        <div className="flex space-x-1">
                                            <span className="animate-pulse-dot bg-gray-400 w-2 h-2 rounded-full"></span>
                                            <span className="animate-pulse-dot-delay bg-gray-400 w-2 h-2 rounded-full"></span>
                                            <span className="animate-pulse-dot-delay-more bg-gray-400 w-2 h-2 rounded-full"></span>
                                        </div>
                                    </div>
                                </div>
                            )}
                            <div ref={messagesEndRef} /> {/* Scroll target */}
                        </div>

                        {/* Chat Input Area */}
                        <div className="p-4 bg-gray-50 border-t border-gray-200 flex items-end space-x-3 rounded-b-xl">
                            <textarea
                                ref={textareaRef}
                                className="flex-1 p-3 border border-gray-300 rounded-xl focus:outline-none focus:ring-1 focus:ring-blue-500 resize-none overflow-hidden placeholder-gray-400 text-sm bg-white shadow-inner"
                                placeholder="Send a message..."
                                value={input}
                                onChange={(e) => setInput(e.target.value)}
                                onKeyPress={handleKeyPress}
                                rows="1"
                                style={{ minHeight: '48px', maxHeight: '160px' }}
                            />
                            <button
                                className="bg-blue-600 text-white p-3 rounded-xl shadow-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2 transition-colors duration-200 flex-shrink-0 flex items-center justify-center"
                                onClick={handleSendMessage}
                                disabled={isTyping || input.trim() === ''}
                            >
                                <svg
                                    xmlns="http://www.w3.org/2000/svg"
                                    fill="none"
                                    viewBox="0 0 24 24"
                                    strokeWidth="1.5"
                                    stroke="currentColor"
                                    className="w-6 h-6"
                                >
                                    <path
                                        strokeLinecap="round"
                                        strokeLinejoin="round"
                                        d="M6 12L3.269 3.126A59.768 59.768 0 0121.485 12 59.77 0 013.27 20.876L5.999 12zm0 0h7.5"
                                    />
                                </svg>
                            </button>
                        </div>
                    </div>
                </div>
            );
        };

        // Render the App component into the 'root' div
        ReactDOM.render(<App />, document.getElementById('root'));
    </script>
</body>
</html>


# --- LangChain & HuggingFace Imports ---
from transformers import AutoTokenizer

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationSummaryBufferMemory
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage

# --- Custom LLM Class with Qwen Token Counter ---
class QwenTokenCountingChatOpenAI(ChatOpenAI):
    tokenizer: AutoTokenizer = None

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        if self.tokenizer is None:
            # Use the exact tokenizer name you are using for your Qwen model
            # Ensure this tokenizer is available in your Kubeflow environment
            tokenizer_name_or_path = "Qwen/Qwen1.5-72B-Chat-AWQ"
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)
            except Exception as e:
                print(f"ERROR: Could not load Qwen tokenizer from '{tokenizer_name_or_path}': {e}")
                # Fallback to a generic tokenizer if Qwen's isn't found
                self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        
        # Ensure chat_template is set for Qwen models
        if not hasattr(self.tokenizer, 'apply_chat_template') or self.tokenizer.chat_template is None:
            self.tokenizer.chat_template = (
                "{% for message in messages %}"
                "{% if message['role'] == 'user' %}"
                "{{ '<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% elif message['role'] == 'assistant' %}"
                "{{ '<|im_start|>assistant\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% elif message['role'] == 'system' %}"
                "{{ '<|im_start|>system\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% else %}"
                "{{ '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% endif %}"
                "{% endfor %}"
                "{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}"
            )

    def _get_num_tokens_from_messages(self, messages: List[BaseMessage]) -> int:
        if self.tokenizer is None:
            return sum(len(msg.content.split()) for msg in messages)

        hf_messages = []
        for msg in messages:
            if isinstance(msg, HumanMessage):
                hf_messages.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                hf_messages.append({"role": "assistant", "content": msg.content})
            elif isinstance(msg, SystemMessage):
                hf_messages.append({"role": "system", "content": msg.content})
            else:
                hf_messages.append({"role": msg.type, "content": msg.content})

        try:
            token_ids = self.tokenizer.apply_chat_template(
                hf_messages,
                tokenize=True,
                add_generation_prompt=True
            )
            return len(token_ids)
        except Exception as e:
            print(f"Warning: Failed to apply chat template for token counting: {e}. Falling back to word count.")
            return sum(len(msg.content.split()) for msg in messages)


# --- Helper Function for Formatting Retrieved Documents ---
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


# --- 1. RAG Setup: Embeddings, Vector Store, and Retriever ---
print("Setting up RAG components...")
# Ensure 'BAAI/bge-small-en-v1.5' model is downloaded/available in your environment
embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en-v1.5")
doc_vectorstore = FAISS.from_documents(
    [
        Document(page_content="The Amazon rainforest is located in South America."),
        Document(page_content="The Amazon river is the largest river by discharge volume."),
        Document(page_content="The sun is a star. It is in the center of the solar system."),
        Document(page_content="The Earth orbits the sun in an elliptical path."),
        Document(page_content="Photosynthesis is the process by which plants convert light energy into chemical energy."),
        Document(page_content="The capital of France is Paris."),
        Document(page_content="The Eiffel Tower is located in Paris."),
        Document(page_content="Mount Everest is the highest mountain in the world.")
    ],
    embeddings
)
retriever = doc_vectorstore.as_retriever(search_kwargs={"k": 2})
print("RAG components ready.")


# --- 2. LLM Initialization ---
print("Initializing LLM...")
# Ensure your vLLM server for Qwen is running at http://localhost:8000/v1
llm = QwenTokenCountingChatOpenAI(
    model="qwen-72b-awq", # This should match the model deployed on your vLLM server
    openai_api_base="http://localhost:8000/v1", # This is the vLLM server URL
    openai_api_key="EMPTY", # vLLM often uses "EMPTY" or doesn't require a key
    max_tokens=2048,
    temperature=0.0, # Keep at 0.0 for deterministic RAG
    stop=["<|im_end|>", "----", "Human:", "\nHuman:", "\n\nHuman:"]
)
print("LLM ready.")


# --- 3. Memory Setup (In-Memory Summary Buffer) ---
# Note: This memory is managed server-side. Each chat session (if you had user IDs)
# would ideally have its own memory instance or be loaded/saved from a database.
# For this simple demo, it's a single global memory for all incoming requests.
print("Setting up memory manager (in-memory)...")
memory_manager = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=1024,
    return_messages=True,
    memory_key="chat_history"
)
print("Memory manager ready.")


# --- 4. Define the RAG Prompt ---
rag_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI assistant. Use the following retrieved context to answer the question.\n"
               "Provide a comprehensive and helpful answer based on the context. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n"
               "Retrieved Context:\n{context}\n\n"
               "Chat History:\n{chat_history_for_prompt}\n\n" # Placeholder for chat history
               "Answer the user's question clearly and concisely."),
    MessagesPlaceholder(variable_name="chat_history_for_prompt"), # This is where the actual messages go
    ("human", "{input}")
])
print("RAG prompt template ready.")


# --- Construct the RAG Chain ---
# This chain combines retrieval, prompt formatting, LLM invocation, and output parsing.
rag_chain = (
    RunnablePassthrough.assign(
        context=itemgetter("input") | retriever | format_docs,
        # 'chat_history_for_prompt' is passed directly from the API request's input dictionary
    )
    | rag_prompt
    | llm
    | StrOutputParser()
)
print("RAG chain constructed.")


# --- FastAPI Application Setup ---
app = FastAPI(
    title="RAG Chatbot Backend",
    description="A simple RAG chatbot API with memory, powered by LangChain and Qwen LLM."
)

# --- CORS Configuration ---
# This is CRUCIAL for allowing your browser-based React app to talk to this API.
# In a Kubeflow environment, your Jupyter Notebook/Lab UI will be served from a specific URL.
# You need to include that URL in the `allow_origins` list.
# For development, allowing all origins (`*`) or `null` (for local file access) can work,
# but for production, specify exact origins.
origins = [
    "http://localhost:3000",  # Common for local React dev server
    "http://localhost:8888",  # Common Jupyter default port
    "http://127.0.0.1:8888",  # Another common Jupyter default
    "null",                   # For local HTML files opened directly in browser (file:// protocol)
    "*",                      # Allow all origins for broad development compatibility (LESS SECURE FOR PROD)
    # Add the specific URL of your Kubeflow Jupyter Notebook/Lab instance here, e.g.:
    # "https://your-kubeflow-domain.com/notebook/your-namespace/your-notebook-name/lab",
    # "https://your-kubeflow-dashboard-url.com"
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],  # Allow all HTTP methods
    allow_headers=["*"],  # Allow all headers
)

@app.post("/chat")
async def chat_endpoint(request: Request):
    """
    API endpoint to receive user messages, process with RAG and LLM,
    and return a chatbot response.
    """
    try:
        data = await request.json()
        user_message_text = data.get("message")
        # Frontend sends history as a list of {sender: 'user'/'bot', text: '...'}
        frontend_chat_history = data.get("chat_history", [])

        if not user_message_text:
            raise HTTPException(status_code=400, detail="Message is required")

        print(f"\n--- Received Request ---")
        print(f"User Message: '{user_message_text}'")
        print(f"Frontend History Length: {len(frontend_chat_history)}")

        # Convert frontend history format to LangChain BaseMessage format
        # This is crucial for the MessagesPlaceholder in the prompt
        langchain_chat_history: List[BaseMessage] = []
        for msg in frontend_chat_history:
            if msg['sender'] == 'user':
                langchain_chat_history.append(HumanMessage(content=msg['text']))
            elif msg['sender'] == 'bot':
                langchain_chat_history.append(AIMessage(content=msg['text']))
        
        # Prepare inputs for the RAG chain
        # 'input' is the current user query.
        # 'chat_history_for_prompt' is the list of previous BaseMessages.
        chain_inputs = {
            "input": user_message_text,
            "chat_history_for_prompt": langchain_chat_history
        }

        # Invoke the RAG chain asynchronously
        print("Invoking RAG chain...")
        bot_response_content = await rag_chain.ainvoke(chain_inputs)
        
        print(f"Bot Response: '{bot_response_content}'")

        # Save the current turn to the memory manager
        # This updates the server-side summary buffer memory.
        memory_manager.save_context(
            {"input": user_message_text},
            {"output": bot_response_content}
        )
        
        return JSONResponse(content={"response": bot_response_content})

    except HTTPException as e:
        print(f"HTTP Error: {e.detail}")
        raise e # Re-raise HTTP exceptions for FastAPI to handle
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        import traceback
        traceback.print_exc() # Print full traceback for debugging
        raise HTTPException(status_code=500, detail="Internal server error")

# Entry point for Uvicorn
if __name__ == "__main__":
    # When running in Kubeflow, the container might expose port 8080 or 8888
    # and Kubeflow's ingress/proxy will map it to the external URL.
    # Ensure this port matches what you configure in Kubeflow's notebook server settings
    # or what you expect the proxy to forward to.
    uvicorn.run(app, host="0.0.0.0", port=8080) # Using 8080 as a common port for Kubeflow services
