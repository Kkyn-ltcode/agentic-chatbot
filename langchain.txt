import os
import asyncio
from operator import itemgetter
from typing import List, Optional, Any, Dict

# --- FastAPI Imports ---
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

# --- LangChain & HuggingFace Imports ---
from transformers import AutoTokenizer

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationSummaryBufferMemory
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage

# --- Custom LLM Class with Qwen Token Counter ---
class QwenTokenCountingChatOpenAI(ChatOpenAI):
    tokenizer: AutoTokenizer = None

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        if self.tokenizer is None:
            # Use the exact tokenizer name you are using for your Qwen model
            # Ensure this tokenizer is available in your Kubeflow environment
            tokenizer_name_or_path = "Qwen/Qwen1.5-72B-Chat-AWQ"
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)
            except Exception as e:
                print(f"ERROR: Could not load Qwen tokenizer from '{tokenizer_name_or_path}': {e}")
                # Fallback to a generic tokenizer if Qwen's isn't found
                self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        
        # Ensure chat_template is set for Qwen models
        if not hasattr(self.tokenizer, 'apply_chat_template') or self.tokenizer.chat_template is None:
            self.tokenizer.chat_template = (
                "{% for message in messages %}"
                "{% if message['role'] == 'user' %}"
                "{{ '<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% elif message['role'] == 'assistant' %}"
                "{{ '<|im_start|>assistant\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% elif message['role'] == 'system' %}"
                "{{ '<|im_start|>system\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% else %}"
                "{{ '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% endif %}"
                "{% endfor %}"
                "{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}"
            )

    def _get_num_tokens_from_messages(self, messages: List[BaseMessage]) -> int:
        if self.tokenizer is None:
            return sum(len(msg.content.split()) for msg in messages)

        hf_messages = []
        for msg in messages:
            if isinstance(msg, HumanMessage):
                hf_messages.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                hf_messages.append({"role": "assistant", "content": msg.content})
            elif isinstance(msg, SystemMessage):
                hf_messages.append({"role": "system", "content": msg.content})
            else:
                hf_messages.append({"role": msg.type, "content": msg.content})

        try:
            token_ids = self.tokenizer.apply_chat_template(
                hf_messages,
                tokenize=True,
                add_generation_prompt=True
            )
            return len(token_ids)
        except Exception as e:
            print(f"Warning: Failed to apply chat template for token counting: {e}. Falling back to word count.")
            return sum(len(msg.content.split()) for msg in messages)


# --- Helper Function for Formatting Retrieved Documents ---
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


# --- 1. RAG Setup: Embeddings, Vector Store, and Retriever ---
print("Setting up RAG components...")
# Ensure 'BAAI/bge-small-en-v1.5' model is downloaded/available in your environment
embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en-v1.5")
doc_vectorstore = FAISS.from_documents(
    [
        Document(page_content="The Amazon rainforest is located in South America."),
        Document(page_content="The Amazon river is the largest river by discharge volume."),
        Document(page_content="The sun is a star. It is in the center of the solar system."),
        Document(page_content="The Earth orbits the sun in an elliptical path."),
        Document(page_content="Photosynthesis is the process by which plants convert light energy into chemical energy."),
        Document(page_content="The capital of France is Paris."),
        Document(page_content="The Eiffel Tower is located in Paris."),
        Document(page_content="Mount Everest is the highest mountain in the world.")
    ],
    embeddings
)
retriever = doc_vectorstore.as_retriever(search_kwargs={"k": 2})
print("RAG components ready.")


# --- 2. LLM Initialization ---
print("Initializing LLM...")
# Ensure your vLLM server for Qwen is running at http://localhost:8000/v1
llm = QwenTokenCountingChatOpenAI(
    model="qwen-72b-awq", # This should match the model deployed on your vLLM server
    openai_api_base="http://localhost:8000/v1", # This is the vLLM server URL
    openai_api_key="EMPTY", # vLLM often uses "EMPTY" or doesn't require a key
    max_tokens=2048,
    temperature=0.0, # Keep at 0.0 for deterministic RAG
    stop=["<|im_end|>", "----", "Human:", "\nHuman:", "\n\nHuman:"]
)
print("LLM ready.")


# --- 3. Memory Setup (In-Memory Summary Buffer) ---
# Note: This memory is managed server-side. Each chat session (if you had user IDs)
# would ideally have its own memory instance or be loaded/saved from a database.
# For this simple demo, it's a single global memory for all incoming requests.
print("Setting up memory manager (in-memory)...")
memory_manager = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=1024,
    return_messages=True,
    memory_key="chat_history"
)
print("Memory manager ready.")


# --- 4. Define the RAG Prompt ---
rag_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI assistant. Use the following retrieved context to answer the question.\n"
               "Provide a comprehensive and helpful answer based on the context. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n"
               "Retrieved Context:\n{context}\n\n"
               "Chat History:\n{chat_history_for_prompt}\n\n" # Placeholder for chat history
               "Answer the user's question clearly and concisely."),
    MessagesPlaceholder(variable_name="chat_history_for_prompt"), # This is where the actual messages go
    ("human", "{input}")
])
print("RAG prompt template ready.")


# --- Construct the RAG Chain ---
# This chain combines retrieval, prompt formatting, LLM invocation, and output parsing.
rag_chain = (
    RunnablePassthrough.assign(
        context=itemgetter("input") | retriever | format_docs,
        # 'chat_history_for_prompt' is passed directly from the API request's input dictionary
    )
    | rag_prompt
    | llm
    | StrOutputParser()
)
print("RAG chain constructed.")


# --- FastAPI Application Setup ---
app = FastAPI(
    title="RAG Chatbot Backend",
    description="A simple RAG chatbot API with memory, powered by LangChain and Qwen LLM."
)

# --- CORS Configuration ---
# This is CRUCIAL for allowing your browser-based React app to talk to this API.
# In a Kubeflow environment, your Jupyter Notebook/Lab UI will be served from a specific URL.
# You need to include that URL in the `allow_origins` list.
# For development, allowing all origins (`*`) or `null` (for local file access) can work,
# but for production, specify exact origins.
origins = [
    "http://localhost:3000",  # Common for local React dev server
    "http://localhost:8888",  # Common Jupyter default port
    "http://127.0.0.1:8888",  # Another common Jupyter default
    "null",                   # For local HTML files opened directly in browser (file:// protocol)
    "*",                      # Allow all origins for broad development compatibility (LESS SECURE FOR PROD)
    # Add the specific URL of your Kubeflow Jupyter Notebook/Lab instance here, e.g.:
    # "https://your-kubeflow-domain.com/notebook/your-namespace/your-notebook-name/lab",
    # "https://your-kubeflow-dashboard-url.com"
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],  # Allow all HTTP methods
    allow_headers=["*"],  # Allow all headers
)

@app.post("/chat")
async def chat_endpoint(request: Request):
    """
    API endpoint to receive user messages, process with RAG and LLM,
    and return a chatbot response.
    """
    try:
        data = await request.json()
        user_message_text = data.get("message")
        # Frontend sends history as a list of {sender: 'user'/'bot', text: '...'}
        frontend_chat_history = data.get("chat_history", [])

        if not user_message_text:
            raise HTTPException(status_code=400, detail="Message is required")

        print(f"\n--- Received Request ---")
        print(f"User Message: '{user_message_text}'")
        print(f"Frontend History Length: {len(frontend_chat_history)}")

        # Convert frontend history format to LangChain BaseMessage format
        # This is crucial for the MessagesPlaceholder in the prompt
        langchain_chat_history: List[BaseMessage] = []
        for msg in frontend_chat_history:
            if msg['sender'] == 'user':
                langchain_chat_history.append(HumanMessage(content=msg['text']))
            elif msg['sender'] == 'bot':
                langchain_chat_history.append(AIMessage(content=msg['text']))
        
        # Prepare inputs for the RAG chain
        # 'input' is the current user query.
        # 'chat_history_for_prompt' is the list of previous BaseMessages.
        chain_inputs = {
            "input": user_message_text,
            "chat_history_for_prompt": langchain_chat_history
        }

        # Invoke the RAG chain asynchronously
        print("Invoking RAG chain...")
        bot_response_content = await rag_chain.ainvoke(chain_inputs)
        
        print(f"Bot Response: '{bot_response_content}'")

        # Save the current turn to the memory manager
        # This updates the server-side summary buffer memory.
        memory_manager.save_context(
            {"input": user_message_text},
            {"output": bot_response_content}
        )
        
        return JSONResponse(content={"response": bot_response_content})

    except HTTPException as e:
        print(f"HTTP Error: {e.detail}")
        raise e # Re-raise HTTP exceptions for FastAPI to handle
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        import traceback
        traceback.print_exc() # Print full traceback for debugging
        raise HTTPException(status_code=500, detail="Internal server error")

# Entry point for Uvicorn
if __name__ == "__main__":
    # When running in Kubeflow, the container might expose port 8080 or 8888
    # and Kubeflow's ingress/proxy will map it to the external URL.
    # Ensure this port matches what you configure in Kubeflow's notebook server settings
    # or what you expect the proxy to forward to.
    uvicorn.run(app, host="0.0.0.0", port=8080) # Using 8080 as a common port for Kubeflow services
