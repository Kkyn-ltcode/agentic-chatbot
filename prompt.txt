czYxam14aXRsN3MyODI1b2gyZG5uOkw2bTBwQTdRbU5kMVFDWTBNOE9BQmREM29xN1o5TUpa

from deepeval.metrics import GEval

correctness_metric = GEval(
    name="Correctness",
    evaluation_type="criteria",
    model="gpt-4",
    
    criteria="""
Evaluate the factual correctness of the chatbot's response compared to the expected answer.
Your task is to judge whether the chatbot output conveys factually accurate and semantically equivalent information.
Focus only on information that is verifiable from the expected answer.
Ignore style, verbosity, or phrasing differences unless they change the meaning.
""",

    evaluation_steps=[
        "Identify all factual claims or key statements made in the chatbot's output.",
        "Compare each claim against the expected answer.",
        "Label each as one of: (1) Fully correct, (2) Partially correct, (3) Incorrect or unsupported.",
        "Determine if any hallucinated (unsupported) or contradictory facts are present.",
        "Summarize the overall factual alignment and assign a score based on the rubric.",
        "Explain why the score was assigned, referencing specific claims."
    ],

    rubric="""
Scoring Rubric for Factual Correctness:

1.0 — Perfect factual alignment. All chatbot claims are fully supported by the expected answer. No incorrect or hallucinated facts.
0.8 — Mostly correct. One minor omission or slight imprecision that doesn’t alter core meaning.
0.6 — Several partially correct statements. Some omissions or mild factual drift.
0.4 — Mix of correct and incorrect. Important facts missing or distorted.
0.2 — Mostly incorrect. One or two weakly related facts but significant contradictions or hallucinations.
0.0 — Entirely incorrect or hallucinated. Contradicts expected answer or fabricates information.

Always provide a reasoned explanation for the score.
"""
)


from deepeval.metrics import GEval

correctness_metric = GEval(
    name="Correctness",
    criteria="Evaluate the factual correctness of the chatbot's answer based on the expected answer.",
    evaluation_type="criteria",
    model="gpt-4",
    evaluation_steps=[
        "Break the actual answer into factual statements.",
        "Compare each with the expected answer and mark as correct, partially correct, or incorrect.",
        "Aggregate your judgment into a final score.",
        "Explain your reasoning."
    ],
    rubric="""
    1.0 - The response is entirely factually correct and aligns fully with the expected answer. No incorrect or misleading statements.
    0.7 - The response is mostly correct but may have minor omissions or slightly imprecise information.
    0.5 - The response is partially correct, mixing correct and incorrect or incomplete statements.
    0.2 - The response contains mostly incorrect or unrelated statements.
    0.0 - The response is completely incorrect or contradicts the ground truth.
    """
)

from deepeval.metrics import GEval

from deepeval.metrics import GEval

answer_relevance_metric = GEval(
    name="Answer Relevance",
    evaluation_type="criteria",
    model="gpt-4",  # Or your local judge model

    criteria=(
        "Evaluate the *relevance* of the chatbot's response to the user's question. "
        "A relevant answer stays focused on the user's intent, avoids unrelated information, "
        "and directly addresses what was asked — even if incomplete or incorrect. "
        "Do NOT assess factual correctness. Only judge whether the answer is on-topic, "
        "focused, and contextually appropriate based on the question."
    ),

    evaluation_steps=[
        "Step 1: Read and fully understand the user's question. Identify the main intent and scope.",
        "Step 2: Read the chatbot's answer. Segment it into statements or ideas.",
        "Step 3: For each segment, judge whether it directly supports or relates to the user's intent.",
        "Step 4: Identify any irrelevant, overly general, or off-topic parts.",
        "Step 5: Based on the proportion and impact of relevant vs. irrelevant content, assign a score using the rubric.",
        "Step 6: Justify your score with clear reference to the question and answer."
    ],

    rubric=(
        "1.0 – The answer is fully relevant. Every part addresses the user's intent clearly and directly.\n"
        "0.8 – Mostly relevant. One or two minor tangents or omissions, but no major distractions.\n"
        "0.6 – Partially relevant. Some content aligns with the question, but significant portions are off-topic or redundant.\n"
        "0.3 – Barely relevant. A few pieces may relate to the question, but most of the answer is not aligned.\n"
        "0.0 – Completely irrelevant. The answer does not address the user's question at all."
    )
)


from deepeval.metrics import GEval

RAGFaithfulnessGEval = GEval(
    name="Faithfulness (RAG)",
    model="gpt-4",  # Or local Claude/LLama3 judge
    evaluation_type="criteria",
    criteria=(
        "You are an expert LLM evaluator. You must verify whether the chatbot's response is entirely grounded in the provided retrieved documents (context). "
        "Your goal is to detect hallucinations: any factual claims not directly supported by the context. "
        "Do not use your own external knowledge. Do not assume. Only trust what's in the context. "
        "Treat any unsupported or fabricated information as a hallucination and mark it clearly."
    ),
    evaluation_steps=[
        "Step 1: Break down the chatbot's response into factual claims or assertions.",
        "Step 2: For each claim, determine whether it is explicitly supported by the retrieved documents.",
        "Step 3: Label each claim as: [Supported], [Partially Supported], or [Unsupported/Hallucinated].",
        "Step 4: List the exact context passage(s) that support each supported claim. If none, explain why it is unsupported.",
        "Step 5: Based on your analysis, assign a score using the rubric below.",
        "Step 6: Provide a detailed explanation justifying your score."
    ],
    rubric="""
    1.0 — All claims are clearly and explicitly supported by the retrieved context. No hallucination.
    0.8 — One minor unsupported detail, but otherwise grounded.
    0.6 — Several unsupported or weakly grounded statements.
    0.4 — Significant hallucination or content not found in context.
    0.2 — Mostly unfaithful; only loosely related to context.
    0.0 — Fully ungrounded; content fabricated or contradicts context.
    """
)

