from deepeval.metrics import GEval

correctness_metric = GEval(
    name="Correctness",
    criteria="Is the chatbot's response factually correct based on the ground-truth answer?",
    evaluation_type="criteria",
    model="gpt-4",
    evaluation_steps=[
        "Break down the actual answer into individual factual statements.",
        "Compare each statement with the expected answer.",
        "Mark each statement as correct, partially correct, or incorrect, with reasoning.",
        "Based on the overall correctness, assign a score between 0.0 and 1.0.",
        "Explain why you assigned this score."
    ]
)

from deepeval.metrics import GEval

correctness_metric = GEval(
    name="Correctness",
    criteria="Evaluate the factual correctness of the chatbot's answer based on the expected answer.",
    evaluation_type="criteria",
    model="gpt-4",
    evaluation_steps=[
        "Break the actual answer into factual statements.",
        "Compare each with the expected answer and mark as correct, partially correct, or incorrect.",
        "Aggregate your judgment into a final score.",
        "Explain your reasoning."
    ],
    rubric="""
    1.0 - The response is entirely factually correct and aligns fully with the expected answer. No incorrect or misleading statements.
    0.7 - The response is mostly correct but may have minor omissions or slightly imprecise information.
    0.5 - The response is partially correct, mixing correct and incorrect or incomplete statements.
    0.2 - The response contains mostly incorrect or unrelated statements.
    0.0 - The response is completely incorrect or contradicts the ground truth.
    """
)

from deepeval.metrics import GEval

answer_relevance_metric = GEval(
    name="Answer Relevance",
    evaluation_type="criteria",
    model="gpt-4",  # or your own local judge model
    criteria=(
        "Evaluate the relevance of the chatbot's response to the user's question. "
        "An answer is relevant if it directly addresses the user's query, stays on-topic, "
        "and avoids unrelated or extraneous information. Do not judge factual accuracy. "
        "Focus only on how well the answer aligns with what the user asked."
    ),
    evaluation_steps=[
        "Read the question and understand what information the user is asking for.",
        "Read the chatbot's answer and identify its main points or statements.",
        "For each point, decide whether it is directly relevant to the question.",
        "Ignore whether the answer is correct — focus only on topicality and focus.",
        "Assign a score based on the rubric, and explain your reasoning."
    ],
    rubric="""
    1.0 - The answer is completely relevant, focused, and directly addresses all parts of the question.
    0.7 - Mostly relevant with slight off-topic content or missing minor parts of the question.
    0.5 - Partially relevant; the core of the question is only loosely addressed.
    0.2 - Mostly irrelevant or off-topic, but has one or two related points.
    0.0 - Completely irrelevant; does not address the question at all.
    """
)

from deepeval.metrics import GEval

RAGFaithfulnessGEval = GEval(
    name="Faithfulness (RAG)",
    model="gpt-4",  # Or local Claude/LLama3 judge
    evaluation_type="criteria",
    criteria=(
        "You are an expert LLM evaluator. You must verify whether the chatbot's response is entirely grounded in the provided retrieved documents (context). "
        "Your goal is to detect hallucinations: any factual claims not directly supported by the context. "
        "Do not use your own external knowledge. Do not assume. Only trust what's in the context. "
        "Treat any unsupported or fabricated information as a hallucination and mark it clearly."
    ),
    evaluation_steps=[
        "Step 1: Break down the chatbot's response into factual claims or assertions.",
        "Step 2: For each claim, determine whether it is explicitly supported by the retrieved documents.",
        "Step 3: Label each claim as: [Supported], [Partially Supported], or [Unsupported/Hallucinated].",
        "Step 4: List the exact context passage(s) that support each supported claim. If none, explain why it is unsupported.",
        "Step 5: Based on your analysis, assign a score using the rubric below.",
        "Step 6: Provide a detailed explanation justifying your score."
    ],
    rubric="""
    1.0 — All claims are clearly and explicitly supported by the retrieved context. No hallucination.
    0.8 — One minor unsupported detail, but otherwise grounded.
    0.6 — Several unsupported or weakly grounded statements.
    0.4 — Significant hallucination or content not found in context.
    0.2 — Mostly unfaithful; only loosely related to context.
    0.0 — Fully ungrounded; content fabricated or contradicts context.
    """
)

