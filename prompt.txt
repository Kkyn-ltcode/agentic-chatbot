from deepeval.metrics import GEval

correctness_metric = GEval(
    name="Correctness",
    criteria="Is the chatbot's response factually correct based on the ground-truth answer?",
    evaluation_type="criteria",
    model="gpt-4",
    evaluation_steps=[
        "Break down the actual answer into individual factual statements.",
        "Compare each statement with the expected answer.",
        "Mark each statement as correct, partially correct, or incorrect, with reasoning.",
        "Based on the overall correctness, assign a score between 0.0 and 1.0.",
        "Explain why you assigned this score."
    ]
)

from deepeval.metrics import GEval

correctness_metric = GEval(
    name="Correctness",
    criteria="Evaluate the factual correctness of the chatbot's answer based on the expected answer.",
    evaluation_type="criteria",
    model="gpt-4",
    evaluation_steps=[
        "Break the actual answer into factual statements.",
        "Compare each with the expected answer and mark as correct, partially correct, or incorrect.",
        "Aggregate your judgment into a final score.",
        "Explain your reasoning."
    ],
    rubric="""
    1.0 - The response is entirely factually correct and aligns fully with the expected answer. No incorrect or misleading statements.
    0.7 - The response is mostly correct but may have minor omissions or slightly imprecise information.
    0.5 - The response is partially correct, mixing correct and incorrect or incomplete statements.
    0.2 - The response contains mostly incorrect or unrelated statements.
    0.0 - The response is completely incorrect or contradicts the ground truth.
    """
)

from deepeval.metrics import GEval

answer_relevance_metric = GEval(
    name="Answer Relevance",
    evaluation_type="criteria",
    model="gpt-4",  # or your own local judge model
    criteria=(
        "Evaluate the relevance of the chatbot's response to the user's question. "
        "An answer is relevant if it directly addresses the user's query, stays on-topic, "
        "and avoids unrelated or extraneous information. Do not judge factual accuracy. "
        "Focus only on how well the answer aligns with what the user asked."
    ),
    evaluation_steps=[
        "Read the question and understand what information the user is asking for.",
        "Read the chatbot's answer and identify its main points or statements.",
        "For each point, decide whether it is directly relevant to the question.",
        "Ignore whether the answer is correct — focus only on topicality and focus.",
        "Assign a score based on the rubric, and explain your reasoning."
    ],
    rubric="""
    1.0 - The answer is completely relevant, focused, and directly addresses all parts of the question.
    0.7 - Mostly relevant with slight off-topic content or missing minor parts of the question.
    0.5 - Partially relevant; the core of the question is only loosely addressed.
    0.2 - Mostly irrelevant or off-topic, but has one or two related points.
    0.0 - Completely irrelevant; does not address the question at all.
    """
)

from deepeval.metrics import GEval

FaithfulnessGEval = GEval(
    name="Faithfulness",
    model="gpt-4",  # Or your own judge LLM
    evaluation_type="criteria",
    criteria=(
        "You are evaluating whether the chatbot's response is **faithful** to the expected answer. "
        "A response is faithful if every piece of information in it is **directly grounded** in the expected answer. "
        "If the chatbot introduces information that is not found in the expected answer — even if it's plausible or true — "
        "it is considered a **hallucination** and reduces the faithfulness score. "
        "Focus only on whether the answer strictly adheres to the content of the expected answer. "
        "Do not judge style, fluency, or helpfulness. Only grounding."
    ),
    evaluation_steps=[
        "Step 1: Break the actual output into clear factual or informative statements.",
        "Step 2: For each statement, determine if it is fully supported, partially supported, or unsupported by the expected answer.",
        "Step 3: Mark any hallucinated, invented, or overly speculative content.",
        "Step 4: Explain each judgment with specific reference to the expected answer.",
        "Step 5: Based on these findings, assign a final score between 0.0 and 1.0 using the rubric."
    ],
    rubric="""
    1.0 — All content in the actual answer is directly grounded in the expected answer. No hallucination.
    0.8 — One very minor unsupported detail or slight elaboration, but overall faithful.
    0.6 — Mostly faithful, but a few unsupported or speculative additions.
    0.4 — Several hallucinated or fabricated details that dilute the original meaning.
    0.2 — Largely unfaithful with major hallucinations or incorrect inferences.
    0.0 — Completely ungrounded; answer is fabricated or contradicts the expected answer.
    """
)
