from deepeval.metrics import GEval

correctness_metric = GEval(
    name="Correctness",
    criteria="Is the chatbot's response factually correct based on the ground-truth answer?",
    evaluation_type="criteria",
    model="gpt-4",
    evaluation_steps=[
        "Break down the actual answer into individual factual statements.",
        "Compare each statement with the expected answer.",
        "Mark each statement as correct, partially correct, or incorrect, with reasoning.",
        "Based on the overall correctness, assign a score between 0.0 and 1.0.",
        "Explain why you assigned this score."
    ]
)

from deepeval.metrics import GEval

correctness_metric = GEval(
    name="Correctness",
    criteria="Evaluate the factual correctness of the chatbot's answer based on the expected answer.",
    evaluation_type="criteria",
    model="gpt-4",
    evaluation_steps=[
        "Break the actual answer into factual statements.",
        "Compare each with the expected answer and mark as correct, partially correct, or incorrect.",
        "Aggregate your judgment into a final score.",
        "Explain your reasoning."
    ],
    rubric="""
    1.0 - The response is entirely factually correct and aligns fully with the expected answer. No incorrect or misleading statements.
    0.7 - The response is mostly correct but may have minor omissions or slightly imprecise information.
    0.5 - The response is partially correct, mixing correct and incorrect or incomplete statements.
    0.2 - The response contains mostly incorrect or unrelated statements.
    0.0 - The response is completely incorrect or contradicts the ground truth.
    """
)

from deepeval.metrics import GEval

answer_relevance_metric = GEval(
    name="Answer Relevance",
    evaluation_type="criteria",
    model="gpt-4",  # or your own local judge model
    criteria=(
        "Evaluate the relevance of the chatbot's response to the user's question. "
        "An answer is relevant if it directly addresses the user's query, stays on-topic, "
        "and avoids unrelated or extraneous information. Do not judge factual accuracy. "
        "Focus only on how well the answer aligns with what the user asked."
    ),
    evaluation_steps=[
        "Read the question and understand what information the user is asking for.",
        "Read the chatbot's answer and identify its main points or statements.",
        "For each point, decide whether it is directly relevant to the question.",
        "Ignore whether the answer is correct â€” focus only on topicality and focus.",
        "Assign a score based on the rubric, and explain your reasoning."
    ],
    rubric="""
    1.0 - The answer is completely relevant, focused, and directly addresses all parts of the question.
    0.7 - Mostly relevant with slight off-topic content or missing minor parts of the question.
    0.5 - Partially relevant; the core of the question is only loosely addressed.
    0.2 - Mostly irrelevant or off-topic, but has one or two related points.
    0.0 - Completely irrelevant; does not address the question at all.
    """
)
