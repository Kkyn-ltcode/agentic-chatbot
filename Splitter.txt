import polars as pl
import multiprocessing as mp
from itertools import product
from pathlib import Path

# --------------------
# CONFIG
# --------------------
INPUT_PATH = "flows.parquet"  # dataset with columns: ID, Src, Dst
OUTPUT_PATH = "edges_global.parquet"
TMP_DIR = Path("edges_tmp")
TMP_DIR.mkdir(exist_ok=True)

# --------------------
# STEP 1: Load and prepare
# --------------------
print("ðŸ”¹ Loading dataset...")
df = pl.read_parquet(INPUT_PATH).select(["ID", "Src", "Dst"])

# Optional: speed up joins and hashing
df = df.with_columns([
    pl.col("Src").cast(pl.Categorical),
    pl.col("Dst").cast(pl.Categorical)
])

# --------------------
# STEP 2: Build global maps (convert to pure Python)
# --------------------
print("ðŸ”¹ Building global Src and Dst maps...")

src_map = (
    df.select(["Src", "ID"])
    .group_by("Src")
    .agg(pl.col("ID").alias("ids"))
)

dst_map = (
    df.select(["Dst", "ID"])
    .group_by("Dst")
    .agg(pl.col("ID").alias("ids"))
)

# Convert Polars Series -> native Python objects
src_dict = {
    k: v.to_list() for k, v in zip(src_map["Src"].to_list(), src_map["ids"])
}
dst_dict = {
    k: v.to_list() for k, v in zip(dst_map["Dst"].to_list(), dst_map["ids"])
}

print(f"âœ… Built {len(src_dict):,} Src keys and {len(dst_dict):,} Dst keys.")

# --------------------
# STEP 3: Edge generation per key
# --------------------
def gen_edges_for_key(args):
    key, src_ids, dst_ids = args
    pairs = []

    if src_ids and len(src_ids) > 1:
        pairs.extend(product(src_ids, src_ids))  # Srcâ€“Src

    if dst_ids and len(dst_ids) > 1:
        pairs.extend(product(dst_ids, dst_ids))  # Dstâ€“Dst

    if src_ids and dst_ids:
        pairs.extend(product(src_ids, dst_ids))  # Srcâ€“Dst
        pairs.extend(product(dst_ids, src_ids))  # Dstâ€“Src

    # remove self-loops
    return [(a, b) for (a, b) in pairs if a != b]

# --------------------
# STEP 4: Merge keyspaces
# --------------------
print("ðŸ”¹ Preparing parallel workload...")
all_keys = set(src_dict.keys()) | set(dst_dict.keys())

def key_arg_generator():
    for key in all_keys:
        yield (key, src_dict.get(key, []), dst_dict.get(key, []))

# --------------------
# STEP 5: Parallel edge generation
# --------------------
print("ðŸ”¹ Generating edges in parallel...")
with mp.Pool(processes=mp.cpu_count()) as pool:
    all_edges = pool.map(gen_edges_for_key, key_arg_generator(), chunksize=500)

# Flatten and build DataFrame
edges = pl.DataFrame(
    [pair for pairs in all_edges for pair in pairs],
    schema=["id1", "id2"]
).unique()

edges.write_parquet(OUTPUT_PATH)
print(f"âœ… Done. Global edges written to {OUTPUT_PATH}")
print(f"Total edges: {edges.shape[0]:,}")
