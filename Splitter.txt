import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
import torch.multiprocessing as mp
from torch_geometric.nn import SAGEConv
from torch_geometric.data import Data
from torch_geometric.loader import GraphSAINTEdgeSampler

# --------------------------------
# 1. Model
# --------------------------------
class EdgeClassifier(nn.Module):
    def __init__(self, in_channels, hidden_channels, edge_in_channels):
        super().__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, hidden_channels)
        self.edge_mlp = nn.Sequential(
            nn.Linear(hidden_channels * 2 + edge_in_channels, 128),
            nn.ReLU(),
            nn.Linear(128, 2)
        )

    def forward(self, x, edge_index, edge_attr):
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        src, dst = edge_index
        edge_emb = torch.cat([x[src], x[dst], edge_attr], dim=1)
        return self.edge_mlp(edge_emb)

# --------------------------------
# 2. Train function
# --------------------------------
def train(rank, world_size, data):
    print(f"[Rank {rank}] Starting training process.")
    os.environ["CUDA_VISIBLE_DEVICES"] = str(rank)
    torch.cuda.set_device(rank)

    dist.init_process_group(
        backend="nccl",
        init_method="env://",
        world_size=world_size,
        rank=rank
    )

    device = torch.device(f"cuda:{rank}")
    model = EdgeClassifier(data.x.size(1), 128, data.edge_attr.size(1)).to(device)
    model = nn.parallel.DistributedDataParallel(model, device_ids=[rank])

    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    loader = GraphSAINTEdgeSampler(
        data,
        batch_size=8000,
        num_steps=10,
        sample_coverage=100,
        shuffle=True
    )

    for epoch in range(5):
        model.train()
        total_loss = 0

        for sub_data in loader:
            sub_data = sub_data.to(device)
            optimizer.zero_grad()

            out = model(sub_data.x, sub_data.edge_index, sub_data.edge_attr)
            loss = F.cross_entropy(
                out[sub_data.edge_mask],
                sub_data.edge_label[sub_data.edge_mask]
            )
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        if rank == 0:  # only main GPU prints
            print(f"Epoch {epoch:02d} | Loss: {total_loss:.4f}")

    dist.destroy_process_group()

# --------------------------------
# 3. Main entry
# --------------------------------
def main():
    print("Inside main() — starting DDP setup")
    world_size = torch.cuda.device_count()
    print(f"Detected {world_size} GPUs")

    # Create dummy data (replace with your actual graph)
    num_nodes, num_edges = 50000, 200000
    data = Data(
        x=torch.randn(num_nodes, 64),
        edge_index=torch.randint(0, num_nodes, (2, num_edges)),
        edge_attr=torch.randn(num_edges, 8),
        edge_label=torch.randint(0, 2, (num_edges,))
    )

    # Required by torch.distributed
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "12355"

    # Spawn one process per GPU
    mp.spawn(train, args=(world_size, data), nprocs=world_size, join=True)


if __name__ == "__main__":
    print("Script started")  # ✅ This will print before torchrun spawns
    main()
