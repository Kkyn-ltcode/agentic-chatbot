from langchain.retrievers import ContextualCompressionRetriever
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_compressors import FlashrankRerank

# 1. Your base retriever with a high k to "cast a wide net"
embeddings = OpenAIEmbeddings()
db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
base_retriever = db.as_retriever(search_kwargs={"k": 20}) # Fetch 20 documents initially

# 2. The reranker model
# Flashrank is an open-source, ultra-fast option. Cohere is another popular choice.
compressor = FlashrankRerank(top_n=5) # Rerank and return only the top 5

# 3. Combine them into a single retriever
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=base_retriever
)

# Use the new retriever, which will automatically fetch, rerank, and filter.
query = "Why did the company's revenue increase?"
docs = compression_retriever.invoke(query)

# The reranker's score is often added to the document's metadata
for doc in docs:
    print(f"Content: {doc.page_content[:150]}...")
    print(f"Reranker Score: {doc.metadata.get('relevance_score')}\n---")
