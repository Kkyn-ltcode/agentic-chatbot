# main.py (or app.py) for Simple RAG Chatbot
import os
import asyncio
from operator import itemgetter
from typing import List, Optional, Any

# FastAPI imports
from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

# --- LangChain Imports ---
from transformers import AutoTokenizer # Used for token counting in QwenTokenCountingChatOpenAI

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI # Used to connect to vLLM's OpenAI-compatible API
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationSummaryBufferMemory
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage

# --- Custom LLM Class for Qwen (to connect to vLLM and for token counting) ---
class QwenTokenCountingChatOpenAI(ChatOpenAI):
    tokenizer: AutoTokenizer = None

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        if self.tokenizer is None:
            tokenizer_name_or_path = "Qwen/Qwen1.5-72B-Chat-AWQ" # Or the actual path/name of your Qwen model
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)
            except Exception as e:
                print(f"ERROR: Could not load Qwen tokenizer from '{tokenizer_name_or_path}': {e}")
                self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased") # Fallback
        
        if not hasattr(self.tokenizer, 'apply_chat_template') or self.tokenizer.chat_template is None:
            self.tokenizer.chat_template = (
                "{% for message in messages %}"
                "{% if message['role'] == 'user' %}"
                "{{ '<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% elif message['role'] == 'assistant' %}"
                "{{ '<|im_start|>assistant\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% elif message['role'] == 'system' %}"
                "{{ '<|im_start|>system\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% else %}"
                "{{ '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% endif %}"
                "{% endfor %}"
                "{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}"
            )

    def _get_num_tokens_from_messages(self, messages: List[BaseMessage]) -> int:
        if self.tokenizer is None:
            return sum(len(msg.content.split()) for msg in messages)

        hf_messages = []
        for msg in messages:
            if isinstance(msg, HumanMessage):
                hf_messages.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                hf_messages.append({"role": "assistant", "content": msg.content})
            elif isinstance(msg, SystemMessage):
                hf_messages.append({"role": "system", "content": msg.content})
            else:
                hf_messages.append({"role": msg.type, "content": msg.content})

        try:
            token_ids = self.tokenizer.apply_chat_template(
                hf_messages,
                tokenize=True,
                add_generation_prompt=True
            )
            return len(token_ids)
        except Exception as e:
            print(f"Warning: Failed to apply chat template for token counting: {e}. Falling back to word count.")
            return sum(len(msg.content.split()) for msg in messages)


# --- Helper Function for Formatting Retrieved Documents ---
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


# --- 1. RAG Setup: Embeddings, Vector Store, and Retriever ---
print("Setting up RAG components...")
embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en-v1.5")
doc_vectorstore = FAISS.from_documents(
    [
        Document(page_content="The Amazon rainforest is located in South America."),
        Document(page_content="The Amazon river is the largest river by discharge volume."),
        Document(page_content="The sun is a star. It is in the center of the solar system."),
        Document(page_content="The Earth orbits the sun in an elliptical path."),
        Document(page_content="Photosynthesis is the process by which plants convert light energy into chemical energy."),
        Document(page_content="The capital of France is Paris."),
        Document(page_content="The Eiffel Tower is located in Paris."),
        Document(page_content="Mount Everest is the highest mountain in the world.")
    ],
    embeddings
)
retriever = doc_vectorstore.as_retriever(search_kwargs={"k": 2})
print("RAG components ready.")


# --- 2. LLM Initialization (Connecting to your vLLM server) ---
print("Initializing LLM (connecting to vLLM)...")
llm = QwenTokenCountingChatOpenAI(
    model="Qwen/Qwen1.5-72B-Chat-AWQ", # This should match the model ID you are serving with vLLM
    openai_api_base="http://localhost:8000/v1", # The default vLLM OpenAI-compatible API endpoint
    openai_api_key="EMPTY", # vLLM typically doesn't require an API key for local serving
    max_tokens=2048,
    temperature=0.0, # Keep at 0.0 for strict adherence
    stop=["<|im_end|>", "----", "Human:", "\nHuman:", "\n\nHuman:"] # Adjust based on Qwen's actual stop tokens if different
)
print("LLM ready.")


# --- 3. Memory Setup (In-Memory Summary Buffer) ---
print("Setting up memory manager (in-memory)...")
memory_manager = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=1024,
    return_messages=True,
    memory_key="chat_history"
)
print("Memory manager ready.")


# --- 4. Define the RAG Chain ---
print("Defining the RAG chain...")
rag_chain_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI assistant. Use the following retrieved context to answer the question. "
                 "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n"
                 "Retrieved Context:\n{context}\n\n"
                 "Chat History:\n{chat_history}"), # Use chat_history here
    ("human", "{question}")
])

rag_chain = (
    RunnablePassthrough.assign(
        context=itemgetter("question") | retriever | format_docs,
        chat_history=RunnableLambda(lambda x: x["chat_history"]) # Pass chat_history directly
    )
    | rag_chain_prompt
    | llm
    | StrOutputParser()
)
print("RAG chain ready.")


# --- FastAPI Application Setup ---
app = FastAPI(
    title="Simple RAG Chatbot Backend",
    description="A simple RAG chatbot backend using LangChain, Hugging Face embeddings, and vLLM.",
    version="1.0.0",
)

# Add CORS middleware to allow requests from your React frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Adjust this to your React app's specific origin in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# In-memory session store for chat history
# IMPORTANT: This is for demonstration. For persistent memory across sessions/users,
# you would integrate a database like Firestore, Redis, etc.
# Memory is NOT persistent across FastAPI server restarts or different users.
session_memories = {}

# Helper to get memory for a session (or create a new one)
def get_session_memory(session_id: str):
    if session_id not in session_memories:
        session_memories[session_id] = ConversationSummaryBufferMemory(
            llm=llm,
            max_token_limit=1024,
            return_messages=True,
            memory_key="chat_history"
        )
    return session_memories[session_id]

# Pydantic model for request body
from pydantic import BaseModel

class ChatRequest(BaseModel):
    user_message: str
    session_id: str = "default_session" # Default session ID for simplicity

@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    user_message = request.user_message
    session_id = request.session_id

    current_memory = get_session_memory(session_id)
    # Load history from memory for the current session
    history_messages = current_memory.load_memory_variables({})["chat_history"]

    full_response_content = ""
    try:
        # Invoke the RAG chain
        response_data = await rag_chain.ainvoke(
            {"question": user_message, "chat_history": history_messages}
        )
        full_response_content = response_data
        
        # Save context after full response is generated
        current_memory.save_context(
            {"input": user_message},
            {"output": full_response_content}
        )
        return {"response": full_response_content}

    except Exception as e:
        print(f"\nAn error occurred during chat processing: {e}")
        error_response = "I apologize, but I encountered an error trying to process that request."
        current_memory.save_context(
            {"input": user_message},
            {"output": error_response}
        )
        raise HTTPException(status_code=500, detail=error_response)

# Health check endpoint
@app.get("/health")
async def health_check():
    return {"status": "ok", "message": "Simple RAG Chatbot Backend is running."}

# To run this FastAPI app, save it as e.g., `main.py`
# Then, in your Kubeflow Notebook terminal, run:
# uvicorn main:app --host 0.0.0.0 --port 8001 --reload
# (Use --reload only for development, remove for production)
