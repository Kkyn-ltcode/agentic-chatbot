import spacy
from transformers import AutoTokenizer
from langchain_core.documents import Document

# Load spaCy for sentence splitting
nlp = spacy.load("en_core_web_sm")  # or "xx_ent_wiki_sm" for multilingual

# Load tokenizer (for Qwen or LLaMA)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen1.5-0.5B", trust_remote_code=True)

# Parameters
chunk_size = 256
chunk_overlap = 32

def split_into_sentence_chunks(text):
    # Step 1: Sentence splitting
    sentences = [sent.text.strip() for sent in nlp(text).sents]

    # Step 2: Merge into token-based chunks
    chunks = []
    current_chunk = []
    current_len = 0

    for sentence in sentences:
        token_count = len(tokenizer.encode(sentence, add_special_tokens=False))

        # If adding this sentence would exceed chunk size
        if current_len + token_count > chunk_size:
            if current_chunk:
                chunks.append(" ".join(current_chunk))
            # Overlap logic (optional)
            if chunk_overlap > 0 and len(chunks) > 0:
                prev = chunks[-1]
                prev_tokens = tokenizer.encode(prev, add_special_tokens=False)
                overlap_tokens = prev_tokens[-chunk_overlap:]
                overlap_text = tokenizer.decode(overlap_tokens)
                current_chunk = [overlap_text]
                current_len = len(overlap_tokens)
            else:
                current_chunk = []
                current_len = 0

        current_chunk.append(sentence)
        current_len += token_count

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return [Document(page_content=chunk) for chunk in chunks]
