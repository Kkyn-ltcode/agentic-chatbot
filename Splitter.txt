import os
import asyncio
from operator import itemgetter
from typing import List, Optional, Any, TypedDict, Union, Annotated
from functools import partial

# --- LangGraph Imports ---
from langgraph.graph import StateGraph, END

# --- Core LangChain Imports ---
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationSummaryBufferMemory
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage

# --- Specific to Qwen Token Counting ---
from transformers import AutoTokenizer


# --- Custom LLM Class with Qwen Token Counter (same as before) ---
class QwenTokenCountingChatOpenAI(ChatOpenAI):
    tokenizer: AutoTokenizer = None

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        if self.tokenizer is None:
            tokenizer_name_or_path = "Qwen/Qwen1.5-72B-Chat-AWQ"
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)
            except Exception as e:
                print(f"ERROR: Could not load Qwen tokenizer from '{tokenizer_name_or_path}': {e}")
                self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        
        if not hasattr(self.tokenizer, 'apply_chat_template') or self.tokenizer.chat_template is None:
            self.tokenizer.chat_template = (
                "{% for message in messages %}"
                "{% if message['role'] == 'user' %}"
                "{{ '<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% elif message['role'] == 'assistant' %}"
                "{{ '<|im_start|>assistant\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% elif message['role'] == 'system' %}"
                "{{ '<|im_start|>system\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% else %}"
                "{{ '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% endif %}"
                "{% endfor %}"
                "{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}"
            )

    def _get_num_tokens_from_messages(self, messages: List[BaseMessage]) -> int:
        if self.tokenizer is None:
            return sum(len(msg.content.split()) for msg in messages)

        hf_messages = []
        for msg in messages:
            if isinstance(msg, HumanMessage):
                hf_messages.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                hf_messages.append({"role": "assistant", "content": msg.content})
            elif isinstance(msg, SystemMessage):
                hf_messages.append({"role": "system", "content": msg.content})
            else:
                hf_messages.append({"role": msg.type, "content": msg.content})

        try:
            token_ids = self.tokenizer.apply_chat_template(
                hf_messages,
                tokenize=True,
                add_generation_prompt=True
            )
            return len(token_ids)
        except Exception as e:
            print(f"Warning: Failed to apply chat template for token counting: {e}. Falling back to word count.")
            return sum(len(msg.content.split()) for msg in messages)


# --- Helper Function for Formatting Retrieved Documents (same as before) ---
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


# --- 1. RAG Setup: Embeddings, Vector Store, and Retriever (same as before) ---
print("Setting up RAG components...")
embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en-v1.5")
doc_vectorstore = FAISS.from_documents(
    [
        Document(page_content="The Amazon rainforest is located in South America."),
        Document(page_content="The Amazon river is the largest river by discharge volume."),
        Document(page_content="The sun is a star. It is in the center of the solar system."),
        Document(page_content="The Earth orbits the sun in an elliptical path."),
        Document(page_content="Photosynthesis is the process by which plants convert light energy into chemical energy."),
        Document(page_content="The capital of France is Paris."),
        Document(page_content="The Eiffel Tower is located in Paris."),
        Document(page_content="Mount Everest is the highest mountain in the world.")
    ],
    embeddings
)
retriever = doc_vectorstore.as_retriever(search_kwargs={"k": 2})
print("RAG components ready.")


# --- 2. LLM Initialization (same as before) ---
print("Initializing LLM...")
llm = QwenTokenCountingChatOpenAI(
    model="qwen-72b-awq",
    openai_api_base="http://localhost:8000/v1",
    openai_api_key="EMPTY",
    max_tokens=2048,
    temperature=0.0,
    stop=["<|im_end|>", "----", "Human:", "\nHuman:", "\n\nHuman:"]
)
print("LLM ready.")


# --- 3. Memory Setup (In-Memory Summary Buffer - same as before) ---
print("Setting up memory manager (in-memory)...")
memory_manager = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=1024,
    return_messages=True,
    memory_key="chat_history"
)
print("Memory manager ready.")


# --- 4. Define the RAG Prompt (SIMPLIFIED for no agents/tools) ---
# This prompt guides the LLM on how to answer based on context and history.
rag_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI assistant. Use the following retrieved context to answer the question.\n"
               "Provide a comprehensive and helpful answer based on the context. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n"
               "Retrieved Context:\n{context}\n\n"
               "Chat History:\n{history}\n\n"
               "Answer the user's question clearly and concisely."), # Added a direct instruction
    MessagesPlaceholder(variable_name="chat_history_for_prompt"), # Will be filled from graph state
    ("human", "{input}") # User's current input
])
print("RAG prompt template ready.")


# --- LangGraph Specific Components ---

# 1. Define the Graph State
# This will be the dict that flows through our graph
class GraphState(TypedDict):
    input: str # The user's current input
    chat_history: List[BaseMessage] # Full chat history
    response: str # The final response generated by the LLM

# 2. Define the Node(s)
def generate_rag_response(state: GraphState) -> GraphState:
    """
    Node that retrieves context, formats the prompt, invokes the LLM,
    and updates the state with the generated response.
    """
    print("\n--- NODE: generate_rag_response ---")
    current_input = state["input"]
    current_chat_history = state["chat_history"]

    # Perform RAG retrieval
    retrieved_docs = retriever.invoke(current_input)
    formatted_context = format_docs(retrieved_docs)

    # Prepare input for the RAG chain
    rag_chain_input = {
        "input": current_input,
        "context": formatted_context,
        "chat_history_for_prompt": current_chat_history # Pass chat_history to the prompt
    }

    # Construct the RAG chain (prompt | llm | parser)
    rag_chain = rag_prompt | llm | StrOutputParser()

    # Invoke the chain
    response_content = rag_chain.invoke(rag_chain_input)
    print(f"Generated Response: {response_content}")

    # Update the state with the generated response
    return {"response": response_content}


# 3. Build the LangGraph Graph
print("Building LangGraph workflow...")
workflow = StateGraph(GraphState)

# Add the single node
workflow.add_node("generate_response", generate_rag_response)

# Set the entry point to our response generation node
workflow.set_entry_point("generate_response")

# From the generate_response node, always end the graph execution
workflow.add_edge("generate_response", END)

# Compile the graph
app = workflow.compile()
print("LangGraph workflow compiled.")


# --- 5. Main Asynchronous Execution Loop ---
async def main():
    print("\nWelcome to the RAG Chatbot with Memory (LangGraph)! Type 'exit' to quit.")
    print("This memory will reset each time the script is restarted.\n")

    while True:
        question = input("You: ")
        if question.lower() == 'exit':
            break

        print("Bot: ", end="", flush=True)
        
        # Load chat history from memory for the *initial* state of the graph
        initial_chat_history = memory_manager.load_memory_variables({})["chat_history"]

        # Prepare initial state for the graph
        inputs = {
            "input": question,
            "chat_history": initial_chat_history,
            "response": "" # Initialize response as empty
        }

        full_response_content = ""
        try:
            # Stream through the graph execution
            # For this simple graph, there's only one step.
            # The 's' here represents the full state after a node runs.
            async for s in app.astream(inputs):
                # We are interested in the 'response' key from the final state.
                # In this simple graph, the 'generate_response' node directly
                # outputs the final response to the state.
                if "response" in s and s["response"]:
                    print(s["response"], end="", flush=True)
                    full_response_content = s["response"]
                    break # Break after the response is generated as it's the only step

            print() # Newline after response

            # Save the full turn to memory manager
            memory_manager.save_context(
                {"input": question},
                {"output": full_response_content}
            )

        except Exception as e:
            print(f"\nAn error occurred: {e}")
            full_response_content = "I apologize, but I encountered an error trying to process that request."
            print(full_response_content)
            memory_manager.save_context(
                {"input": question},
                {"output": full_response_content}
            )

if __name__ == "__main__":
    asyncio.run(main())
