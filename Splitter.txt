import os
import asyncio
from operator import itemgetter
from typing import List, Optional, Any
from functools import partial

# --- Agent and Tool Specific Imports ---
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.agents import AgentExecutor
from langchain.tools import Tool, tool
from langchain.chains import LLMMathChain

# --- Existing RAG and Memory Imports ---
from transformers import AutoTokenizer

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationSummaryBufferMemory
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage

# --- Imports for manual agent construction ---
from langchain.agents.format_scratchpad import format_log_to_messages
from langchain.agents.output_parsers import ReActSingleInputOutputParser


# --- Custom LLM Class with Qwen Token Counter ---
class QwenTokenCountingChatOpenAI(ChatOpenAI):
    tokenizer: AutoTokenizer = None

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        if self.tokenizer is None:
            tokenizer_name_or_path = "Qwen/Qwen1.5-72B-Chat-AWQ"
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)
            except Exception as e:
                print(f"ERROR: Could not load Qwen tokenizer from '{tokenizer_name_or_path}': {e}")
                self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        
        if not hasattr(self.tokenizer, 'apply_chat_template') or self.tokenizer.chat_template is None:
            self.tokenizer.chat_template = (
                "{% for message in messages %}"
                "{% if message['role'] == 'user' %}"
                "{{ '<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% elif message['role'] == 'assistant' %}"
                "{{ '<|im_start|>assistant\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% elif message['role'] == 'system' %}"
                "{{ '<|im_start|>system\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% else %}"
                "{{ '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% endif %}"
                "{% endfor %}"
                "{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}"
            )

    def _get_num_tokens_from_messages(self, messages: List[BaseMessage]) -> int:
        if self.tokenizer is None:
            return sum(len(msg.content.split()) for msg in messages)

        hf_messages = []
        for msg in messages:
            if isinstance(msg, HumanMessage):
                hf_messages.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                hf_messages.append({"role": "assistant", "content": msg.content})
            elif isinstance(msg, SystemMessage):
                hf_messages.append({"role": "system", "content": msg.content})
            else:
                hf_messages.append({"role": msg.type, "content": msg.content})

        try:
            token_ids = self.tokenizer.apply_chat_template(
                hf_messages,
                tokenize=True,
                add_generation_prompt=True
            )
            return len(token_ids)
        except Exception as e:
            print(f"Warning: Failed to apply chat template for token counting: {e}. Falling back to word count.")
            return sum(len(msg.content.split()) for msg in messages)


# --- Helper Function for Formatting Retrieved Documents ---
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


# --- 1. RAG Setup: Embeddings, Vector Store, and Retriever ---
print("Setting up RAG components...")
embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en-v1.5")
doc_vectorstore = FAISS.from_documents(
    [
        Document(page_content="The Amazon rainforest is located in South America."),
        Document(page_content="The Amazon river is the largest river by discharge volume."),
        Document(page_content="The sun is a star. It is in the center of the solar system."),
        Document(page_content="The Earth orbits the sun in an elliptical path."),
        Document(page_content="Photosynthesis is the process by which plants convert light energy into chemical energy."),
        Document(page_content="The capital of France is Paris."),
        Document(page_content="The Eiffel Tower is located in Paris."),
        Document(page_content="Mount Everest is the highest mountain in the world.")
    ],
    embeddings
)
retriever = doc_vectorstore.as_retriever(search_kwargs={"k": 2})
print("RAG components ready.")


# --- 2. LLM Initialization ---
print("Initializing LLM...")
llm = QwenTokenCountingChatOpenAI(
    model="qwen-72b-awq",
    openai_api_base="http://localhost:8000/v1",
    openai_api_key="EMPTY",
    max_tokens=2048,
    temperature=0.7,
    stop=["<|im_end|>", "----", "Human:", "\nHuman:", "\n\nHuman:"]
)
print("LLM ready.")


# --- 3. Memory Setup (In-Memory Summary Buffer) ---
print("Setting up memory manager (in-memory)...")
memory_manager = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=1024,
    return_messages=True,
    memory_key="chat_history"
)
print("Memory manager ready.")


# --- 4. Define Tools for the Agent ---
print("Defining tools for the agent...")

search_tool = DuckDuckGoSearchRun(
    name="web_search",
    description="Useful for answering questions about current events or general knowledge that is NOT in the provided knowledge base."
)

math_chain = LLMMathChain.from_llm(llm=llm, verbose=False)
calculator_tool = Tool(
    name="Calculator",
    func=math_chain.run,
    description="Useful for when you need to answer questions about math or perform calculations."
)

@tool
def knowledge_base_search(query: str) -> str:
    """
    Useful for answering questions about specific predefined knowledge,
    such as facts about the Amazon rainforest, the sun, or geographical data
    that might be in the bot's internal knowledge base.
    Always prioritize using this tool for factual questions that seem
    like they should be in the bot's existing documents.
    """
    history_messages = memory_manager.load_memory_variables({})["chat_history"]

    rag_tool_chain = (
        RunnablePassthrough.assign(
            context=itemgetter("question") | retriever | format_docs,
            history=RunnableLambda(lambda x: history_messages)
        )
        | ChatPromptTemplate.from_messages([
            ("system", "You are a helpful AI assistant. Use the following retrieved context to answer the question. "
                       "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n"
                       "Retrieved Context:\n{context}\n\n"
                       "Chat History:\n{history}"),
            ("human", "{question}")
        ])
        | llm
        | StrOutputParser()
    )
    
    result = rag_tool_chain.invoke({"question": query})
    return result

tools = [search_tool, calculator_tool, knowledge_base_search]
print("Tools defined:", [t.name for t in tools])


# --- 5. Define the Agent Prompt ---
agent_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI assistant. You have access to the following tools: {tools}\n"
               "You should only use the tools if necessary and for factual information. "
               "Prioritize using 'knowledge_base_search' for facts that seem to be in a static knowledge base, "
               "and 'web_search' for current events or general knowledge.\n"
               "\n"
               "Follow this exact format:\n\n"
               "Question: the input question you must answer\n"
               "Thought: you should always think about what to do\n"
               "Action: the action to take, should be one of [{tool_names}]\n"
               "Action Input: the input to the action\n"
               "Observation: the result of the action\n"
               "...\n"
               "Thought: I now know the final answer\n"
               "Final Answer: the final answer to the original input question"),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])
print("Agent prompt template ready.")


# --- Debugging Function for agent_scratchpad (Optional) ---
def debug_scratchpad_type(input_dict: dict) -> dict:
    scratchpad_value = input_dict.get("agent_scratchpad")
    print("\n--- DEBUG: agent_scratchpad BEFORE prompt usage ---")
    print(f"Value: {scratchpad_value!r}")
    print(f"Type: {type(scratchpad_value)}")
    if isinstance(scratchpad_value, list):
        print(f"Is list of BaseMessage: {all(isinstance(msg, BaseMessage) for msg in scratchpad_value)}")
    else:
        print("Not a list.")
    print("--------------------------------------------------\n")
    return input_dict

# --- NEW Debugging Function for LLM Output ---
def debug_llm_output(llm_response: Any) -> Any:
    """
    Prints the raw output from the LLM before it is parsed by ReActSingleInputOutputParser.
    This helps determine if the LLM is generating the 'Final Answer:' correctly.
    """
    # Assuming llm_response is a BaseMessage or has a .content attribute
    output_content = llm_response.content if hasattr(llm_response, 'content') else str(llm_response)
    print("\n--- DEBUG: Raw LLM Output Before Parsing ---")
    print(f"Type of LLM Response Object: {type(llm_response)}")
    print(f"Content: {output_content!r}") # Use !r for raw representation
    print("------------------------------------------\n")
    return llm_response # Pass the original response object along

# --- 6. Create the Agent and Executor ---
print("Creating agent and executor...")

agent_chain = (
    RunnablePassthrough.assign(
        agent_scratchpad=lambda x: format_log_to_messages(x["intermediate_steps"]),
        tools=lambda x: x["tools"],
        tool_names=lambda x: [t.name for t in x["tools"]],
        chat_history=lambda x: x["chat_history"],
        input=lambda x: x["input"],
    )
    | RunnableLambda(debug_scratchpad_type) # Debug for prompt inputs
    | agent_prompt
    | llm.bind(stop=["\nObservation"]) # Stop token for the LLM to signal action/thought
    | RunnableLambda(debug_llm_output) # <-- NEW DEBUG: See what LLM actually generated
    | ReActSingleInputOutputParser() # Parses LLM output into AgentAction/AgentFinish
)


agent_executor = AgentExecutor(
    agent=agent_chain,
    tools=tools,
    verbose=True, # Keep verbose=True to see the agent's full thought process
    handle_parsing_errors=True
)
print("Agent executor ready.")


# --- 7. Main Asynchronous Execution Loop ---
async def main():
    print("\nWelcome to the RAG Chatbot with Agents and In-Memory Summary! Type 'exit' to quit.")
    print("This memory will reset each time the script is restarted.\n")

    while True:
        question = input("You: ")
        if question.lower() == 'exit':
            break

        print("Bot: ", end="", flush=True)
        
        history_for_agent = memory_manager.load_memory_variables({})["chat_history"]

        full_response_content = ""
        try:
            async for chunk in agent_executor.astream(
                {"input": question, "chat_history": history_for_agent, "tools": tools, "tool_names": [t.name for t in tools]}
            ):
                if "output" in chunk:
                    print(chunk["output"], end="", flush=True)
                    full_response_content += chunk["output"]
            print()

            memory_manager.save_context(
                {"input": question},
                {"output": full_response_content}
            )

        except Exception as e:
            print(f"\nAn error occurred: {e}")
            full_response_content = "I apologize, but I encountered an error trying to process that request."
            print(full_response_content)
            memory_manager.save_context(
                {"input": question},
                {"output": full_response_content}
            )

if __name__ == "__main__":
    asyncio.run(main())
