import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch import nn, optim
from torch.utils.data import DataLoader, DistributedSampler, TensorDataset
from tqdm import tqdm


# -----------------------------
# 1. Setup distributed backend
# -----------------------------
def setup(rank, world_size):
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "12355"
    dist.init_process_group(backend="nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)


def cleanup():
    dist.destroy_process_group()


# -----------------------------
# 2. Define a heavier CNN model
# -----------------------------
class ConvNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(256, 512, kernel_size=3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(512 * 8 * 8, 1024),
            nn.ReLU(),
            nn.Linear(1024, 10),
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x


# -----------------------------
# 3. Training loop
# -----------------------------
def train(rank, world_size):
    setup(rank, world_size)

    # === Create random heavy dataset (100k fake images) ===
    x = torch.randn(100_000, 3, 64, 64)
    y = torch.randint(0, 10, (100_000,))
    dataset = TensorDataset(x, y)
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)
    dataloader = DataLoader(dataset, batch_size=128, sampler=sampler, num_workers=4, pin_memory=True)

    # === Model, loss, optimizer ===
    model = ConvNet().to(rank)
    model = nn.parallel.DistributedDataParallel(model, device_ids=[rank])
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-3)

    # === Training ===
    num_epochs = 10
    for epoch in range(num_epochs):
        sampler.set_epoch(epoch)
        running_loss = 0.0

        progress = tqdm(dataloader, disable=(rank != 0), desc=f"Epoch {epoch+1}/{num_epochs}")
        for batch_x, batch_y in progress:
            batch_x, batch_y = batch_x.to(rank, non_blocking=True), batch_y.to(rank, non_blocking=True)
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        avg_loss = torch.tensor(running_loss / len(dataloader), device=rank)
        dist.all_reduce(avg_loss, op=dist.ReduceOp.SUM)
        avg_loss /= world_size

        if rank == 0:
            print(f"[Epoch {epoch+1}] Avg Loss: {avg_loss.item():.4f}")

    cleanup()


# -----------------------------
# 4. Spawn multiple processes
# -----------------------------
def main():
    world_size = torch.cuda.device_count()
    print(f"Using {world_size} GPUs for training.")
    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)


if __name__ == "__main__":
    main()
