import torch
import numpy as np

def analyze_features(x):
    """
    Comprehensive analysis of feature tensor to determine optimal normalization.
    
    Args:
        x: torch.Tensor of shape (N, 43) - your node features
    
    Returns:
        dict: Analysis results
    """
    print("=" * 80)
    print("FEATURE TENSOR ANALYSIS")
    print("=" * 80)
    
    results = {}
    
    # Basic info
    print(f"\nShape: {x.shape}")
    print(f"Dtype: {x.dtype}")
    print(f"Device: {x.device}")
    
    # Convert to float for analysis if needed
    if x.dtype != torch.float32 and x.dtype != torch.float64:
        x = x.float()
    
    # Per-feature statistics
    print("\n" + "=" * 80)
    print("PER-FEATURE STATISTICS")
    print("=" * 80)
    
    for i in range(x.shape[1]):
        feat = x[:, i]
        
        # Basic stats
        min_val = feat.min().item()
        max_val = feat.max().item()
        mean_val = feat.mean().item()
        std_val = feat.std().item()
        median_val = feat.median().item()
        
        # Check for special values
        n_nan = torch.isnan(feat).sum().item()
        n_inf = torch.isinf(feat).sum().item()
        n_zero = (feat == 0).sum().item()
        n_unique = len(torch.unique(feat))
        
        # Distribution checks
        q25 = torch.quantile(feat, 0.25).item()
        q75 = torch.quantile(feat, 0.75).item()
        iqr = q75 - q25
        
        print(f"\n--- Feature {i} ---")
        print(f"  Range: [{min_val:.6f}, {max_val:.6f}]")
        print(f"  Mean: {mean_val:.6f}, Std: {std_val:.6f}")
        print(f"  Median: {median_val:.6f}")
        print(f"  Q25: {q25:.6f}, Q75: {q75:.6f}, IQR: {iqr:.6f}")
        print(f"  Unique values: {n_unique}")
        print(f"  Zeros: {n_zero} ({100*n_zero/len(feat):.2f}%)")
        
        if n_nan > 0:
            print(f"  ⚠️  NaN values: {n_nan}")
        if n_inf > 0:
            print(f"  ⚠️  Inf values: {n_inf}")
        
        # Detect feature type
        if n_unique == 2:
            print(f"  → Type: BINARY")
        elif n_unique <= 10:
            print(f"  → Type: CATEGORICAL (low cardinality)")
        elif std_val == 0:
            print(f"  → Type: CONSTANT")
        elif abs(std_val) < 1e-6:
            print(f"  → Type: NEAR-CONSTANT")
        else:
            # Check scale
            if abs(max_val - min_val) > 1000:
                print(f"  → Type: CONTINUOUS (LARGE SCALE)")
            elif abs(max_val - min_val) < 0.01:
                print(f"  → Type: CONTINUOUS (SMALL SCALE)")
            else:
                print(f"  → Type: CONTINUOUS")
            
            # Check distribution
            if abs(mean_val - median_val) / (std_val + 1e-8) > 1:
                print(f"  → Distribution: SKEWED")
            else:
                print(f"  → Distribution: ROUGHLY SYMMETRIC")
    
    # Global statistics
    print("\n" + "=" * 80)
    print("GLOBAL STATISTICS")
    print("=" * 80)
    
    global_min = x.min().item()
    global_max = x.max().item()
    global_mean = x.mean().item()
    global_std = x.std().item()
    
    print(f"\nGlobal min: {global_min:.6f}")
    print(f"Global max: {global_max:.6f}")
    print(f"Global mean: {global_mean:.6f}")
    print(f"Global std: {global_std:.6f}")
    print(f"Total NaN: {torch.isnan(x).sum().item()}")
    print(f"Total Inf: {torch.isinf(x).sum().item()}")
    
    # Check if already normalized
    print("\n" + "=" * 80)
    print("NORMALIZATION CHECKS")
    print("=" * 80)
    
    if abs(global_min) < 1e-6 and abs(global_max - 1) < 1e-6:
        print("\n✓ Data appears to be in [0, 1] range")
    elif abs(global_min + 1) < 1e-6 and abs(global_max - 1) < 1e-6:
        print("\n✓ Data appears to be in [-1, 1] range")
    elif abs(global_mean) < 0.1 and abs(global_std - 1) < 0.1:
        print("\n✓ Data appears to be standardized (mean≈0, std≈1)")
    else:
        print("\n✗ Data is not normalized")
    
    print("\n" + "=" * 80)
    print("RECOMMENDATIONS")
    print("=" * 80)
    print("\nBased on the analysis above, I'll recommend:")
    print("1. Whether to use StandardScaler, MinMaxScaler, or RobustScaler")
    print("2. Whether to normalize per-feature or globally")
    print("3. How to handle special features (binary, categorical, constant)")
    print("4. Whether any features should be log-transformed first")
    
    return results


# Example usage:
# x = torch.randn(16_000_000, 43)  # Your actual data
# results = analyze_features(x)

print("\n" + "=" * 80)
print("INSTRUCTIONS")
print("=" * 80)
print("""
To use this script:

1. Load your tensor x (shape: 16mil x 43)
2. Run: analyze_features(x)
3. Share the complete output with me
4. I'll provide specific normalization code based on the results

Note: If 16M rows is too much memory, you can sample:
    x_sample = x[torch.randperm(len(x))[:100000]]  # 100k random samples
    analyze_features(x_sample)
""")
