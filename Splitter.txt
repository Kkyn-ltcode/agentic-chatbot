import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import TransformerConv, LayerNorm
from torch_geometric.loader import LinkNeighborLoader
from torch_geometric.data import Data
import math


class EdgeFeatureEncoder(nn.Module):
    """Encode edge features from node pairs"""
    def __init__(self, node_dim, edge_dim, hidden_dim):
        super().__init__()
        self.edge_encoder = nn.Sequential(
            nn.Linear(2 * node_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, edge_dim)
        )
    
    def forward(self, x, edge_index):
        # Concatenate source and target node features
        src, dst = edge_index
        edge_features = torch.cat([x[src], x[dst]], dim=-1)
        return self.edge_encoder(edge_features)


class GraphTransformerLayer(nn.Module):
    """Single Graph Transformer layer with edge awareness"""
    def __init__(self, in_channels, out_channels, heads=8, dropout=0.1, 
                 edge_dim=None, use_edge_attr=True):
        super().__init__()
        
        self.transformer_conv = TransformerConv(
            in_channels=in_channels,
            out_channels=out_channels // heads,
            heads=heads,
            dropout=dropout,
            edge_dim=edge_dim if use_edge_attr else None,
            beta=True,  # Gating mechanism
            root_weight=True
        )
        
        self.norm1 = LayerNorm(out_channels)
        self.norm2 = LayerNorm(out_channels)
        
        # FFN
        self.ffn = nn.Sequential(
            nn.Linear(out_channels, out_channels * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(out_channels * 4, out_channels),
            nn.Dropout(dropout)
        )
        
    def forward(self, x, edge_index, edge_attr=None):
        # Multi-head attention with residual
        x_residual = x
        x = self.transformer_conv(x, edge_index, edge_attr=edge_attr)
        
        # Handle dimension mismatch for first layer
        if x_residual.size(-1) != x.size(-1):
            x_residual = F.pad(x_residual, (0, x.size(-1) - x_residual.size(-1)))
        
        x = self.norm1(x + x_residual)
        
        # FFN with residual
        x = self.norm2(x + self.ffn(x))
        
        return x


class SOTAGraphTransformer(nn.Module):
    """
    State-of-the-art Graph Transformer for Edge Classification
    
    Features:
    - Multi-head attention with edge features
    - Residual connections and layer normalization
    - Virtual node for global information
    - Edge-level predictions with advanced aggregation
    """
    def __init__(
        self,
        num_node_features=39,
        hidden_dim=256,
        num_layers=6,
        num_heads=8,
        num_classes=10,
        dropout=0.1,
        use_virtual_node=True
    ):
        super().__init__()
        
        self.num_layers = num_layers
        self.hidden_dim = hidden_dim
        self.use_virtual_node = use_virtual_node
        
        # Input projection
        self.node_encoder = nn.Sequential(
            nn.Linear(num_node_features, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # Edge feature encoder
        edge_dim = hidden_dim // 2
        self.edge_feature_encoder = EdgeFeatureEncoder(
            hidden_dim, edge_dim, hidden_dim
        )
        
        # Virtual node (for global information)
        if use_virtual_node:
            self.virtual_node_embedding = nn.Embedding(1, hidden_dim)
            self.virtual_node_mlp = nn.ModuleList([
                nn.Sequential(
                    nn.Linear(hidden_dim, hidden_dim * 2),
                    nn.BatchNorm1d(hidden_dim * 2),
                    nn.ReLU(),
                    nn.Dropout(dropout),
                    nn.Linear(hidden_dim * 2, hidden_dim)
                ) for _ in range(num_layers)
            ])
        
        # Graph Transformer layers
        self.transformer_layers = nn.ModuleList([
            GraphTransformerLayer(
                in_channels=hidden_dim,
                out_channels=hidden_dim,
                heads=num_heads,
                dropout=dropout,
                edge_dim=edge_dim,
                use_edge_attr=True
            ) for _ in range(num_layers)
        ])
        
        # Edge classification head
        self.edge_classifier = nn.Sequential(
            nn.Linear(hidden_dim * 3 + edge_dim, hidden_dim * 2),
            nn.LayerNorm(hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, num_classes)
        )
        
        self._reset_parameters()
    
    def _reset_parameters(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, x, edge_index, edge_label_index=None, batch=None):
        """
        Args:
            x: Node features [num_nodes, num_node_features]
            edge_index: Edge connectivity [2, num_edges]
            edge_label_index: Edges to predict [2, num_target_edges]
            batch: Batch vector [num_nodes] (for batched graphs)
        
        Returns:
            edge_logits: Edge predictions [num_target_edges, num_classes]
        """
        # Encode nodes
        x = self.node_encoder(x)
        
        # Encode edge features from node pairs
        edge_attr = self.edge_feature_encoder(x, edge_index)
        
        # Virtual node setup
        if self.use_virtual_node:
            virtual_node = self.virtual_node_embedding(
                torch.zeros(1, dtype=torch.long, device=x.device)
            )
            if batch is None:
                batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)
        
        # Apply transformer layers
        for i, layer in enumerate(self.transformer_layers):
            # Virtual node update
            if self.use_virtual_node:
                # Aggregate node features to virtual node
                virtual_node_input = torch.zeros_like(virtual_node)
                virtual_node_input = virtual_node_input.index_add_(
                    0, batch, x
                ) / (batch.bincount().float().unsqueeze(-1) + 1e-8)
                
                virtual_node = virtual_node + self.virtual_node_mlp[i](
                    virtual_node_input
                )
                
                # Broadcast virtual node to all nodes
                x = x + virtual_node[batch]
            
            # Apply transformer layer
            x = layer(x, edge_index, edge_attr)
        
        # Edge classification
        if edge_label_index is None:
            edge_label_index = edge_index
        
        # Get features for edge endpoints
        src_idx, dst_idx = edge_label_index
        src_features = x[src_idx]
        dst_features = x[dst_idx]
        
        # Compute edge features for target edges
        edge_features = self.edge_feature_encoder(x, edge_label_index)
        
        # Combine features: [src || dst || src*dst || edge_features]
        edge_repr = torch.cat([
            src_features,
            dst_features,
            src_features * dst_features,  # Interaction term
            edge_features
        ], dim=-1)
        
        # Classify edges
        edge_logits = self.edge_classifier(edge_repr)
        
        return edge_logits


class NetworkAttackDetector:
    """
    Wrapper class for training and inference
    """
    def __init__(
        self,
        num_node_features=39,
        hidden_dim=256,
        num_layers=6,
        num_heads=8,
        num_classes=10,
        dropout=0.1,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    ):
        self.device = device
        self.model = SOTAGraphTransformer(
            num_node_features=num_node_features,
            hidden_dim=hidden_dim,
            num_layers=num_layers,
            num_heads=num_heads,
            num_classes=num_classes,
            dropout=dropout
        ).to(device)
        
        self.num_classes = num_classes
    
    def create_dataloader(
        self,
        data,
        edge_label_index,
        edge_label,
        batch_size=1024,
        num_neighbors=[15, 10, 5],
        shuffle=True,
        num_workers=4
    ):
        """
        Create LinkNeighborLoader for edge classification
        
        Args:
            data: PyG Data object with x, edge_index
            edge_label_index: Edges to predict [2, num_edges]
            edge_label: Labels for edges [num_edges]
            batch_size: Number of edges per batch
            num_neighbors: Number of neighbors to sample per layer
            shuffle: Whether to shuffle
            num_workers: Number of data loading workers
        """
        loader = LinkNeighborLoader(
            data,
            num_neighbors=num_neighbors,
            edge_label_index=edge_label_index,
            edge_label=edge_label,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            persistent_workers=True if num_workers > 0 else False,
            pin_memory=True
        )
        return loader
    
    def train_epoch(self, loader, optimizer, criterion):
        """Train for one epoch"""
        self.model.train()
        total_loss = 0
        total_correct = 0
        total_samples = 0
        
        for batch in loader:
            batch = batch.to(self.device)
            optimizer.zero_grad()
            
            # Forward pass
            logits = self.model(
                batch.x,
                batch.edge_index,
                batch.edge_label_index,
                batch.batch if hasattr(batch, 'batch') else None
            )
            
            # Compute loss
            loss = criterion(logits, batch.edge_label)
            
            # Backward pass
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            optimizer.step()
            
            # Metrics
            total_loss += loss.item() * logits.size(0)
            pred = logits.argmax(dim=-1)
            total_correct += (pred == batch.edge_label).sum().item()
            total_samples += logits.size(0)
        
        return total_loss / total_samples, total_correct / total_samples
    
    @torch.no_grad()
    def evaluate(self, loader):
        """Evaluate the model"""
        self.model.eval()
        total_correct = 0
        total_samples = 0
        all_preds = []
        all_labels = []
        
        for batch in loader:
            batch = batch.to(self.device)
            
            logits = self.model(
                batch.x,
                batch.edge_index,
                batch.edge_label_index,
                batch.batch if hasattr(batch, 'batch') else None
            )
            
            pred = logits.argmax(dim=-1)
            total_correct += (pred == batch.edge_label).sum().item()
            total_samples += logits.size(0)
            
            all_preds.append(pred.cpu())
            all_labels.append(batch.edge_label.cpu())
        
        accuracy = total_correct / total_samples
        all_preds = torch.cat(all_preds)
        all_labels = torch.cat(all_labels)
        
        return accuracy, all_preds, all_labels
    
    def fit(
        self,
        train_loader,
        val_loader,
        num_epochs=100,
        lr=1e-3,
        weight_decay=1e-5,
        patience=10
    ):
        """
        Train the model with early stopping
        """
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=lr,
            weight_decay=weight_decay
        )
        
        # Learning rate scheduler
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', factor=0.5, patience=5, verbose=True
        )
        
        # Class weights for imbalanced dataset
        criterion = nn.CrossEntropyLoss()
        
        best_val_acc = 0
        patience_counter = 0
        
        for epoch in range(num_epochs):
            # Train
            train_loss, train_acc = self.train_epoch(
                train_loader, optimizer, criterion
            )
            
            # Validate
            val_acc, _, _ = self.evaluate(val_loader)
            
            # Scheduler step
            scheduler.step(val_acc)
            
            print(f'Epoch {epoch+1}/{num_epochs}:')
            print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')
            print(f'  Val Acc: {val_acc:.4f}')
            
            # Early stopping
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                # Save best model
                torch.save(self.model.state_dict(), 'best_model.pt')
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f'Early stopping at epoch {epoch+1}')
                    break
        
        # Load best model
        self.model.load_state_dict(torch.load('best_model.pt'))
        print(f'Best validation accuracy: {best_val_acc:.4f}')
        
        return best_val_acc


# Example usage
if __name__ == '__main__':
    # Create dummy data (replace with your actual data)
    num_nodes = 1000
    num_edges = 5000
    num_node_features = 39
    num_classes = 10
    
    # Create a sample graph
    x = torch.randn(num_nodes, num_node_features)
    edge_index = torch.randint(0, num_nodes, (2, num_edges))
    
    # Split edges for train/val/test
    num_train = int(0.7 * num_edges)
    num_val = int(0.15 * num_edges)
    
    perm = torch.randperm(num_edges)
    train_edge_idx = perm[:num_train]
    val_edge_idx = perm[num_train:num_train+num_val]
    test_edge_idx = perm[num_train+num_val:]
    
    train_edge_label_index = edge_index[:, train_edge_idx]
    train_edge_label = torch.randint(0, num_classes, (num_train,))
    
    val_edge_label_index = edge_index[:, val_edge_idx]
    val_edge_label = torch.randint(0, num_classes, (num_val,))
    
    # Create data object
    data = Data(x=x, edge_index=edge_index)
    
    # Initialize detector
    detector = NetworkAttackDetector(
        num_node_features=39,
        hidden_dim=256,
        num_layers=6,
        num_heads=8,
        num_classes=10,
        dropout=0.1
    )
    
    # Create dataloaders
    train_loader = detector.create_dataloader(
        data, train_edge_label_index, train_edge_label,
        batch_size=1024, num_neighbors=[15, 10, 5], shuffle=True
    )
    
    val_loader = detector.create_dataloader(
        data, val_edge_label_index, val_edge_label,
        batch_size=1024, num_neighbors=[15, 10, 5], shuffle=False
    )
    
    # Train model
    print("Starting training...")
    detector.fit(train_loader, val_loader, num_epochs=50, lr=1e-3)
