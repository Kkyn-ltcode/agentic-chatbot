async def _generate_new_summary_if_needed(temp_memory, current_user_message, current_bot_message):
    """
    A helper function to explicitly force a summary if the buffer is over the limit.
    This is the key to fixing the bug.
    """
    all_messages = temp_memory.chat_memory.messages + [current_user_message, current_bot_message]
    num_tokens = temp_memory.llm.get_num_tokens_from_messages(all_messages)
    
    if num_tokens > temp_memory.max_token_limit:
        print("Token limit exceeded, generating new summary...")
        new_summary = await temp_memory.predict_new_summary(all_messages, temp_memory.moving_summary_buffer)
        temp_memory.moving_summary_buffer = new_summary
        print(f"New summary generated: {new_summary}")
        return new_summary
    else:
        return temp_memory.moving_summary_buffer
