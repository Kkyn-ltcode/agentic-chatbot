import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import HeteroData
from torch_geometric.nn import Linear, HeteroConv, TransformerConv
from torch_geometric.loader import NeighborLoader
import polars as pl
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.distributed as dist
import torch.multiprocessing as mp
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score
import numpy as np
from tqdm import tqdm
import os
import warnings
from collections import defaultdict
warnings.filterwarnings('ignore')


class EdgeTypeSelector(nn.Module):
    """Learns which edge types are important for each sample"""
    
    def __init__(self, hidden_channels, num_edge_types=3):
        super().__init__()
        self.edge_gate = nn.Sequential(
            nn.Linear(hidden_channels, hidden_channels // 2),
            nn.ReLU(),
            nn.Linear(hidden_channels // 2, num_edge_types),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, x):
        """
        Args:
            x: Node features [num_nodes, hidden_channels]
        Returns:
            edge_weights: [num_nodes, num_edge_types]
        """
        return self.edge_gate(x)


class HaltingUnit(nn.Module):
    """Decides whether to stop processing at current layer"""
    
    def __init__(self, hidden_channels):
        super().__init__()
        self.halting_net = nn.Sequential(
            nn.Linear(hidden_channels, hidden_channels // 4),
            nn.ReLU(),
            nn.Linear(hidden_channels // 4, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        """
        Args:
            x: Node features [num_nodes, hidden_channels]
        Returns:
            halting_prob: [num_nodes, 1] probability to stop
        """
        return self.halting_net(x)


class DynamicExpertLayer(nn.Module):
    """Single expert with dynamic depth and edge selection"""
    
    def __init__(self, hidden_channels, num_heads, dropout, edge_types, num_layers=3):
        super().__init__()
        self.num_layers = num_layers
        self.hidden_channels = hidden_channels
        
        # Graph Transformer layers
        self.convs = nn.ModuleList()
        self.norms = nn.ModuleList()
        self.edge_selectors = nn.ModuleList()
        self.halting_units = nn.ModuleList()
        
        for _ in range(num_layers):
            # Heterogeneous convolution for each edge type
            conv_dict = {}
            for edge_type in edge_types:
                conv_dict[edge_type] = TransformerConv(
                    hidden_channels,
                    hidden_channels // num_heads,
                    heads=num_heads,
                    dropout=dropout,
                    edge_dim=None,
                    beta=True
                )
            
            self.convs.append(HeteroConv(conv_dict, aggr='sum'))
            self.norms.append(nn.LayerNorm(hidden_channels))
            self.edge_selectors.append(EdgeTypeSelector(hidden_channels))
            self.halting_units.append(HaltingUnit(hidden_channels))
    
    def forward(self, x_dict, edge_index_dict, class_labels=None, training=True):
        """
        Forward pass with dynamic depth and edge selection
        
        Returns:
            output: Final node representations
            aux_outputs: Dictionary with auxiliary information
        """
        # Track auxiliary information
        layer_outputs = []
        halting_probs = []
        edge_weights_per_layer = []
        depths = torch.zeros(x_dict['flow'].size(0), device=x_dict['flow'].device)
        cumulative_halting = torch.zeros(x_dict['flow'].size(0), device=x_dict['flow'].device)
        
        current_x = x_dict['flow']
        
        for layer_idx, (conv, norm, edge_selector, halting_unit) in enumerate(
            zip(self.convs, self.norms, self.edge_selectors, self.halting_units)
        ):
            # Compute edge weights for this layer
            edge_weights = edge_selector(current_x)  # [num_nodes, 3]
            edge_weights_per_layer.append(edge_weights)
            
            # Apply weighted edge convolution
            x_dict_input = {'flow': current_x}
            messages = {}
            
            for edge_idx, edge_type in enumerate(edge_index_dict.keys()):
                # Get message from this edge type
                edge_msg = conv.convs[edge_type](
                    current_x, 
                    edge_index_dict[edge_type]
                )
                
                # Weight by learned edge importance
                weight = edge_weights[:, edge_idx].unsqueeze(-1)
                messages[edge_type] = edge_msg * weight
            
            # Aggregate weighted messages
            aggregated = sum(messages.values())
            
            # Residual connection and normalization
            current_x = norm(aggregated + current_x)
            
            # ReLU for hidden layers
            if layer_idx < self.num_layers - 1:
                current_x = F.relu(current_x)
            
            # Store output for this layer
            layer_outputs.append(current_x)
            
            # Compute halting probability
            halt_prob = halting_unit(current_x).squeeze(-1)  # [num_nodes]
            halting_probs.append(halt_prob)
            
            # Update cumulative halting
            cumulative_halting = cumulative_halting + halt_prob
            
            # Track depth (which layer each node reaches)
            still_processing = (cumulative_halting < 1.0).float()
            depths = depths + still_processing
            
            # In training, use all layers; in inference, can early exit
            if not training and layer_idx < self.num_layers - 1:
                # Check if all nodes want to halt
                if (cumulative_halting >= 1.0).all():
                    break
        
        # Weighted combination of layer outputs based on halting
        # For simplicity, use final output but track depth
        output = current_x
        
        aux_outputs = {
            'depths': depths,
            'halting_probs': halting_probs,
            'edge_weights': edge_weights_per_layer,
            'cumulative_halting': cumulative_halting
        }
        
        return output, aux_outputs


class ProtocolRouter(nn.Module):
    """Routes samples to appropriate experts based on learned protocol patterns"""
    
    def __init__(self, in_channels, num_experts, hidden_dim=128):
        super().__init__()
        self.num_experts = num_experts
        
        # Router network
        self.router = nn.Sequential(
            nn.Linear(in_channels, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, num_experts)
        )
    
    def forward(self, x, top_k=2):
        """
        Args:
            x: Input features [num_nodes, in_channels]
            top_k: Number of experts to select
        Returns:
            expert_weights: [num_nodes, num_experts]
            expert_indices: [num_nodes, top_k]
        """
        # Compute routing logits
        logits = self.router(x)  # [num_nodes, num_experts]
        
        # Softmax for load balancing
        expert_probs = F.softmax(logits, dim=-1)
        
        # Select top-k experts
        top_k_weights, top_k_indices = torch.topk(expert_probs, top_k, dim=-1)
        
        # Renormalize weights
        top_k_weights = top_k_weights / top_k_weights.sum(dim=-1, keepdim=True)
        
        return expert_probs, top_k_weights, top_k_indices


class DynamicHeteroGraphTransformer(nn.Module):
    """
    Dynamic Heterogeneous Graph Transformer with:
    - Mixture of Experts
    - Dynamic Depth (Adaptive Computation)
    - Edge Type Selection
    """
    
    def __init__(self, in_channels, hidden_channels, out_channels, 
                 num_experts=6, experts_per_sample=2, num_layers=3,
                 num_heads=4, dropout=0.3, edge_types=None):
        super().__init__()
        
        self.num_experts = num_experts
        self.experts_per_sample = experts_per_sample
        self.num_layers = num_layers
        self.dropout = dropout
        self.edge_types = edge_types
        
        # Shared input projection
        self.input_proj = Linear(in_channels, hidden_channels)
        
        # Protocol router
        self.router = ProtocolRouter(hidden_channels, num_experts)
        
        # Expert networks
        self.experts = nn.ModuleList([
            DynamicExpertLayer(
                hidden_channels=hidden_channels,
                num_heads=num_heads,
                dropout=dropout,
                edge_types=edge_types,
                num_layers=num_layers
            )
            for _ in range(num_experts)
        ])
        
        # Shared output projection
        self.output_proj = nn.Sequential(
            Linear(hidden_channels, hidden_channels // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            Linear(hidden_channels // 2, out_channels)
        )
        
        # Auxiliary classifiers for each layer (for early exit supervision)
        self.aux_classifiers = nn.ModuleList([
            nn.Linear(hidden_channels, out_channels)
            for _ in range(num_layers)
        ])
    
    def forward(self, x_dict, edge_index_dict, class_labels=None, 
                training=True, return_aux=False):
        """
        Forward pass through dynamic MoE architecture
        
        Returns:
            output: Final predictions
            aux_info: Auxiliary information for loss computation
        """
        # Initial projection
        x_dict = {key: self.input_proj(x) for key, x in x_dict.items()}
        flow_features = x_dict['flow']
        
        # Route to experts
        expert_probs, expert_weights, expert_indices = self.router(
            flow_features, 
            top_k=self.experts_per_sample
        )
        
        # Process through selected experts
        num_nodes = flow_features.size(0)
        expert_outputs = torch.zeros(
            num_nodes, 
            self.experts[0].hidden_channels,
            device=flow_features.device
        )
        
        # Collect auxiliary information
        all_depths = []
        all_edge_weights = []
        all_halting_probs = []
        
        # Process each node through its top-k experts
        for expert_idx in range(self.num_experts):
            # Find nodes that should use this expert
            mask = (expert_indices == expert_idx).any(dim=1)
            
            if mask.sum() == 0:
                continue
            
            # Get weights for this expert
            expert_weight = torch.zeros(num_nodes, device=flow_features.device)
            for k in range(self.experts_per_sample):
                expert_mask = expert_indices[:, k] == expert_idx
                expert_weight[expert_mask] = expert_weights[expert_mask, k]
            
            # Create subgraph for this expert's nodes
            x_dict_sub = {'flow': flow_features}
            
            # Forward through expert
            expert_out, aux_out = self.experts[expert_idx](
                x_dict_sub,
                edge_index_dict,
                class_labels=class_labels,
                training=training
            )
            
            # Weighted accumulation
            expert_outputs += expert_out * expert_weight.unsqueeze(-1)
            
            # Collect auxiliary info
            all_depths.append(aux_out['depths'])
            all_edge_weights.append(aux_out['edge_weights'])
            all_halting_probs.append(aux_out['halting_probs'])
        
        # Final output projection
        output = self.output_proj(expert_outputs)
        
        # Prepare auxiliary information
        aux_info = {
            'expert_probs': expert_probs,
            'expert_indices': expert_indices,
            'expert_weights': expert_weights,
            'depths': torch.stack(all_depths).mean(dim=0) if all_depths else None,
            'edge_weights': all_edge_weights,
            'halting_probs': all_halting_probs
        }
        
        if return_aux:
            return output, aux_info
        return output


def compute_dynamic_loss(outputs, labels, aux_info, config, class_frequencies=None):
    """
    Compute total loss including:
    - Classification loss
    - Depth penalty (encourage early exit)
    - Load balancing loss (balance expert usage)
    """
    # Main classification loss
    cls_loss = F.cross_entropy(outputs, labels)
    
    # Depth penalty (encourage efficiency)
    if aux_info['depths'] is not None:
        depths = aux_info['depths']
        
        # Class-aware depth penalty
        if class_frequencies is not None:
            # No penalty for rare classes
            depth_penalty_weights = torch.ones_like(depths)
            for i, freq in enumerate(class_frequencies):
                class_mask = (labels == i)
                if freq < 0.001:  # Rare class (< 0.1%)
                    depth_penalty_weights[class_mask] = 0.0
                elif freq < 0.01:  # Uncommon class (< 1%)
                    depth_penalty_weights[class_mask] = 0.5
            
            depth_loss = (depths * depth_penalty_weights).mean()
        else:
            depth_loss = depths.mean()
        
        depth_loss = config.get('lambda_depth', 0.01) * depth_loss
    else:
        depth_loss = 0.0
    
    # Load balancing loss (encourage balanced expert usage)
    expert_probs = aux_info['expert_probs']
    avg_expert_usage = expert_probs.mean(dim=0)
    uniform_distribution = torch.ones_like(avg_expert_usage) / len(avg_expert_usage)
    
    # KL divergence from uniform distribution
    load_balance_loss = F.kl_div(
        avg_expert_usage.log(),
        uniform_distribution,
        reduction='batchmean'
    )
    load_balance_loss = config.get('lambda_load_balance', 0.01) * load_balance_loss
    
    # Total loss
    total_loss = cls_loss + depth_loss + load_balance_loss
    
    return total_loss, {
        'cls_loss': cls_loss.item(),
        'depth_loss': depth_loss.item() if isinstance(depth_loss, torch.Tensor) else depth_loss,
        'load_balance_loss': load_balance_loss.item()
    }


def load_and_prepare_data(feature_cols):
    """Load graph data and prepare train/val/test splits"""
    print("Loading data...")
    nodes = pl.read_parquet("graph/nodes_features.parquet")
    edges1 = pl.read_parquet("graph/edges_type1.parquet")
    edges2 = pl.read_parquet("graph/edges_type2.parquet")
    edges3 = pl.read_parquet("graph/edges_type3.parquet")
    
    # Create heterogeneous graph
    data = HeteroData()
    data['flow'].x = torch.tensor(nodes.select(feature_cols).to_numpy(), dtype=torch.float32)
    data['flow'].y = torch.tensor(nodes['label'].to_numpy(), dtype=torch.long)
    
    # Add edges
    data['flow', 'same_source', 'flow'].edge_index = torch.tensor(
        [edges1['u'].to_numpy(), edges1['v'].to_numpy()], dtype=torch.long
    )
    data['flow', 'same_dest', 'flow'].edge_index = torch.tensor(
        [edges2['u'].to_numpy(), edges2['v'].to_numpy()], dtype=torch.long
    )
    data['flow', 'bidirectional', 'flow'].edge_index = torch.tensor(
        [edges3['u'].to_numpy(), edges3['v'].to_numpy()], dtype=torch.long
    )
    
    # Create train/val/test masks
    num_nodes = data['flow'].x.size(0)
    indices = np.arange(num_nodes)
    labels = nodes['label'].to_numpy()
    
    # Stratified split
    train_idx, test_idx = train_test_split(
        indices, test_size=0.2, random_state=42, stratify=labels
    )
    train_idx, val_idx = train_test_split(
        train_idx, test_size=0.1, random_state=42,
        stratify=labels[train_idx]
    )
    
    # Create masks
    data['flow'].train_mask = torch.zeros(num_nodes, dtype=torch.bool)
    data['flow'].val_mask = torch.zeros(num_nodes, dtype=torch.bool)
    data['flow'].test_mask = torch.zeros(num_nodes, dtype=torch.bool)
    
    data['flow'].train_mask[train_idx] = True
    data['flow'].val_mask[val_idx] = True
    data['flow'].test_mask[test_idx] = True
    
    # Compute class frequencies for depth penalty
    unique, counts = np.unique(labels, return_counts=True)
    class_frequencies = counts / len(labels)
    data['flow'].class_frequencies = torch.tensor(class_frequencies, dtype=torch.float32)
    
    print(f"Total nodes: {num_nodes:,}")
    print(f"Train: {len(train_idx):,}, Val: {len(val_idx):,}, Test: {len(test_idx):,}")
    print(f"Edge types: same_source={edges1.shape[0]:,}, same_dest={edges2.shape[0]:,}, bidirectional={edges3.shape[0]:,}")
    print(f"\nClass distribution:")
    for cls, freq in enumerate(class_frequencies):
        print(f"  Class {cls}: {freq*100:.2f}%")
    
    return data


def setup_ddp(rank, world_size):
    """Initialize distributed training"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)


def cleanup_ddp():
    """Cleanup distributed training"""
    dist.destroy_process_group()


def train_epoch(model, loader, optimizer, device, rank, config, epoch, class_frequencies):
    """Train for one epoch"""
    model.train()
    total_loss = 0
    total_cls_loss = 0
    total_depth_loss = 0
    total_lb_loss = 0
    total_correct = 0
    total_samples = 0
    
    # Track depth statistics
    depth_stats = defaultdict(list)
    expert_usage = defaultdict(int)
    
    if rank == 0:
        loader = tqdm(loader, desc="Training")
    
    # Check if we should enable dynamics
    enable_dynamics = epoch >= config.get('enable_dynamics_epoch', 30)
    
    for batch in loader:
        batch = batch.to(device)
        optimizer.zero_grad()
        
        # Forward pass
        mask = batch['flow'].train_mask
        if mask.sum() == 0:
            continue
        
        output, aux_info = model(
            batch.x_dict, 
            batch.edge_index_dict,
            class_labels=batch['flow'].y[mask],
            training=True,
            return_aux=True
        )
        
        # Compute loss with dynamic components
        if enable_dynamics:
            loss, loss_dict = compute_dynamic_loss(
                output[mask],
                batch['flow'].y[mask],
                aux_info,
                config,
                class_frequencies=class_frequencies.to(device) if class_frequencies is not None else None
            )
        else:
            # Warmup phase: only classification loss
            loss = F.cross_entropy(output[mask], batch['flow'].y[mask])
            loss_dict = {'cls_loss': loss.item(), 'depth_loss': 0.0, 'load_balance_loss': 0.0}
        
        # Backward pass
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        
        # Metrics
        pred = output[mask].argmax(dim=1)
        correct = (pred == batch['flow'].y[mask]).sum().item()
        
        batch_samples = mask.sum().item()
        total_loss += loss.item() * batch_samples
        total_cls_loss += loss_dict['cls_loss'] * batch_samples
        total_depth_loss += loss_dict['depth_loss'] * batch_samples
        total_lb_loss += loss_dict['load_balance_loss'] * batch_samples
        total_correct += correct
        total_samples += batch_samples
        
        # Track depth per class
        if enable_dynamics and aux_info['depths'] is not None:
            depths = aux_info['depths'][mask].cpu().numpy()
            labels = batch['flow'].y[mask].cpu().numpy()
            for label, depth in zip(labels, depths):
                depth_stats[int(label)].append(float(depth))
        
        # Track expert usage
        if enable_dynamics:
            expert_indices = aux_info['expert_indices'][mask]
            for idx in expert_indices.flatten().cpu().numpy():
                expert_usage[int(idx)] += 1
    
    # Compute average metrics
    avg_loss = total_loss / total_samples
    avg_cls_loss = total_cls_loss / total_samples
    avg_depth_loss = total_depth_loss / total_samples
    avg_lb_loss = total_lb_loss / total_samples
    accuracy = total_correct / total_samples
    
    # Compute average depth per class
    avg_depth_per_class = {
        cls: np.mean(depths) if depths else 0.0
        for cls, depths in depth_stats.items()
    }
    
    return (avg_loss, accuracy, avg_cls_loss, avg_depth_loss, avg_lb_loss, 
            avg_depth_per_class, expert_usage)


@torch.no_grad()
def evaluate(model, loader, device, rank, mask_name='val_mask', config=None):
    """Evaluate model"""
    model.eval()
    total_loss = 0
    total_correct = 0
    total_samples = 0
    all_preds = []
    all_labels = []
    
    # Track dynamics
    depth_stats = defaultdict(list)
    edge_weight_stats = defaultdict(list)
    
    if rank == 0:
        loader = tqdm(loader, desc=f"Evaluating ({mask_name})")
    
    for batch in loader:
        batch = batch.to(device)
        
        # Forward pass
        mask = getattr(batch['flow'], mask_name)
        
        if mask.sum() == 0:
            continue
        
        output, aux_info = model(
            batch.x_dict,
            batch.edge_index_dict,
            training=False,
            return_aux=True
        )
        
        loss = F.cross_entropy(output[mask], batch['flow'].y[mask])
        pred = output[mask].argmax(dim=1)
        
        total_loss += loss.item() * mask.sum().item()
        total_correct += (pred == batch['flow'].y[mask]).sum().item()
        total_samples += mask.sum().item()
        
        all_preds.extend(pred.cpu().numpy())
        all_labels.extend(batch['flow'].y[mask].cpu().numpy())
        
        # Track depth per class
        if aux_info['depths'] is not None:
            depths = aux_info['depths'][mask].cpu().numpy()
            labels = batch['flow'].y[mask].cpu().numpy()
            for label, depth in zip(labels, depths):
                depth_stats[int(label)].append(float(depth))
        
        # Track edge weights per class
        if aux_info['edge_weights']:
            for layer_edge_weights in aux_info['edge_weights']:
                if layer_edge_weights is not None:
                    edge_weights = layer_edge_weights[mask].cpu().numpy()
                    labels = batch['flow'].y[mask].cpu().numpy()
                    for label, weights in zip(labels, edge_weights):
                        if int(label) not in edge_weight_stats:
                            edge_weight_stats[int(label)] = []
                        edge_weight_stats[int(label)].append(weights)
    
    avg_loss = total_loss / total_samples if total_samples > 0 else 0
    accuracy = total_correct / total_samples if total_samples > 0 else 0
    
    # Calculate F1 scores
    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)
    f1_weighted = f1_score(all_labels, all_preds, average='weighted', zero_division=0)
    
    # Average depth per class
    avg_depth_per_class = {
        cls: np.mean(depths) if depths else 0.0
        for cls, depths in depth_stats.items()
    }
    
    # Average edge weights per class [same_source, same_dest, bidirectional]
    avg_edge_weights_per_class = {
        cls: np.mean(weights, axis=0) if weights else np.zeros(3)
        for cls, weights in edge_weight_stats.items()
    }
    
    return (avg_loss, accuracy, f1_macro, f1_weighted, all_preds, all_labels,
            avg_depth_per_class, avg_edge_weights_per_class)


def train_worker(rank, world_size, data, feature_cols, config):
    """Training worker for each GPU"""
    
    # Setup DDP
    setup_ddp(rank, world_size)
    device = torch.device(f'cuda:{rank}')
    
    # Create neighbor loaders for mini-batch training
    train_loader = NeighborLoader(
        data,
        num_neighbors=[15, 10, 5],
        batch_size=config['batch_size'],
        input_nodes=('flow', data['flow'].train_mask),
        shuffle=True,
        num_workers=4,
        persistent_workers=True
    )
    
    val_loader = NeighborLoader(
        data,
        num_neighbors=[15, 10, 5],
        batch_size=config['batch_size'] * 2,
        input_nodes=('flow', data['flow'].val_mask),
        shuffle=False,
        num_workers=4,
        persistent_workers=True
    )
    
    # Initialize model
    edge_types = [
        ('flow', 'same_source', 'flow'),
        ('flow', 'same_dest', 'flow'),
        ('flow', 'bidirectional', 'flow')
    ]
    
    model = DynamicHeteroGraphTransformer(
        in_channels=len(feature_cols),
        hidden_channels=config['hidden_channels'],
        out_channels=config['num_classes'],
        num_experts=config['num_experts'],
        experts_per_sample=config['experts_per_sample'],
        num_layers=config['num_layers'],
        num_heads=config['num_heads'],
        dropout=config['dropout'],
        edge_types=edge_types
    ).to(device)
    
    # Wrap model with DDP
    model = DDP(model, device_ids=[rank], find_unused_parameters=True)
    
    # Optimizer and scheduler
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config['learning_rate'],
        weight_decay=config['weight_decay']
    )
    
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=config['epochs'],
        eta_min=1e-6
    )
    
    # Get class frequencies
    class_frequencies = data['flow'].class_frequencies if hasattr(data['flow'], 'class_frequencies') else None
    
    # Training loop
    best_val_f1 = 0
    patience_counter = 0
    
    for epoch in range(config['epochs']):
        if rank == 0:
            print(f"\n{'='*70}")
            print(f"Epoch {epoch + 1}/{config['epochs']}")
            if epoch < config.get('warmup_epochs', 10):
                print("Phase: WARMUP (no dynamics)")
            elif epoch < config.get('enable_dynamics_epoch', 30):
                print("Phase: ENABLE DYNAMICS")
            else:
                print("Phase: FULL DYNAMIC TRAINING")
            print(f"{'='*70}")
        
        # Train
        train_results = train_epoch(
            model, train_loader, optimizer, device, rank, config, epoch, class_frequencies
        )
        train_loss, train_acc, cls_loss, depth_loss, lb_loss, depth_per_class, expert_usage = train_results
        
        # Validate
        val_results = evaluate(model, val_loader, device, rank, 'val_mask', config)
        val_loss, val_acc, val_f1_macro, val_f1_weighted, _, _, val_depth_per_class, val_edge_weights = val_results
        
        scheduler.step()
        
        if rank == 0:
            print(f"\nüìä Training Metrics:")
            print(f"  Total Loss: {train_loss:.4f}")
            print(f"  Classification Loss: {cls_loss:.4f}")
            print(f"  Depth Loss: {depth_loss:.4f}")
            print(f"  Load Balance Loss: {lb_loss:.4f}")
            print(f"  Accuracy: {train_acc:.4f}")
            
            print(f"\nüìä Validation Metrics:")
            print(f"  Loss: {val_loss:.4f}")
            print(f"  Accuracy: {val_acc:.4f}")
            print(f"  F1 (Macro): {val_f1_macro:.4f}")
            print(f"  F1 (Weighted): {val_f1_weighted:.4f}")
            print(f"  Learning Rate: {scheduler.get_last_lr()[0]:.6f}")
            
            # Print depth statistics
            if depth_per_class:
                print(f"\nüìà Average Depth per Class (Validation):")
                class_names = config.get('class_names', [f'Class_{i}' for i in range(config['num_classes'])])
                for cls in sorted(val_depth_per_class.keys()):
                    class_name = class_names[cls] if cls < len(class_names) else f'Class_{cls}'
                    print(f"  {class_name}: {val_depth_per_class[cls]:.2f} layers")
            
            # Print edge weight statistics
            if val_edge_weights:
                print(f"\nüîó Edge Weights per Class (Validation):")
                print(f"  {'Class':<15} {'Same_Src':<12} {'Same_Dst':<12} {'Bidir':<12}")
                print(f"  {'-'*51}")
                class_names = config.get('class_names', [f'Class_{i}' for i in range(config['num_classes'])])
                for cls in sorted(val_edge_weights.keys()):
                    class_name = class_names[cls] if cls < len(class_names) else f'Class_{cls}'
                    weights = val_edge_weights[cls]
                    print(f"  {class_name:<15} {weights[0]:<12.3f} {weights[1]:<12.3f} {weights[2]:<12.3f}")
            
            # Print expert usage
            if expert_usage:
                print(f"\nü§ñ Expert Usage (Training):")
                total_usage = sum(expert_usage.values())
                for expert_id in sorted(expert_usage.keys()):
                    usage_pct = (expert_usage[expert_id] / total_usage) * 100 if total_usage > 0 else 0
                    print(f"  Expert {expert_id}: {usage_pct:.1f}%")
            
            # Save best model
            if val_f1_weighted > best_val_f1:
                best_val_f1 = val_f1_weighted
                patience_counter = 0
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.module.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'val_f1': val_f1_weighted,
                    'config': config,
                    'depth_per_class': val_depth_per_class,
                    'edge_weights_per_class': val_edge_weights
                }, 'best_model.pt')
                print(f"\n‚úì Saved best model (F1: {best_val_f1:.4f})")
            else:
                patience_counter += 1
            
            # Early stopping
            if patience_counter >= config['patience']:
                print(f"\n‚ö† Early stopping triggered after {epoch + 1} epochs")
                break
    
    # Test evaluation on rank 0
    if rank == 0:
        print(f"\n{'='*70}")
        print("üéØ Final Evaluation on Test Set")
        print(f"{'='*70}")
        
        # Load best model
        checkpoint = torch.load('best_model.pt')
        model.module.load_state_dict(checkpoint['model_state_dict'])
        
        test_loader = NeighborLoader(
            data,
            num_neighbors=[15, 10, 5],
            batch_size=config['batch_size'] * 2,
            input_nodes=('flow', data['flow'].test_mask),
            shuffle=False,
            num_workers=4,
            persistent_workers=True
        )
        
        test_results = evaluate(model, test_loader, device, rank, 'test_mask', config)
        test_loss, test_acc, test_f1_macro, test_f1_weighted, preds, labels, test_depth_per_class, test_edge_weights = test_results
        
        print(f"\nüìä Test Metrics:")
        print(f"  Loss: {test_loss:.4f}")
        print(f"  Accuracy: {test_acc:.4f}")
        print(f"  F1 (Macro): {test_f1_macro:.4f}")
        print(f"  F1 (Weighted): {test_f1_weighted:.4f}")
        
        # Print depth statistics
        if test_depth_per_class:
            print(f"\nüìà Average Depth per Class (Test):")
            class_names = config.get('class_names', [f'Class_{i}' for i in range(config['num_classes'])])
            for cls in sorted(test_depth_per_class.keys()):
                class_name = class_names[cls] if cls < len(class_names) else f'Class_{cls}'
                print(f"  {class_name}: {test_depth_per_class[cls]:.2f} layers")
            
            # Calculate expected speedup
            avg_depth = np.mean(list(test_depth_per_class.values()))
            speedup = config['num_layers'] / avg_depth if avg_depth > 0 else 1.0
            print(f"\n‚ö° Performance Gain:")
            print(f"  Average Depth: {avg_depth:.2f} / {config['num_layers']} layers")
            print(f"  Expected Speedup: {speedup:.2f}x")
        
        # Print edge weight statistics
        if test_edge_weights:
            print(f"\nüîó Edge Weights per Class (Test):")
            print(f"  {'Class':<15} {'Same_Src':<12} {'Same_Dst':<12} {'Bidir':<12} {'Dominant':<12}")
            print(f"  {'-'*63}")
            class_names = config.get('class_names', [f'Class_{i}' for i in range(config['num_classes'])])
            edge_names = ['Same_Src', 'Same_Dst', 'Bidir']
            for cls in sorted(test_edge_weights.keys()):
                class_name = class_names[cls] if cls < len(class_names) else f'Class_{cls}'
                weights = test_edge_weights[cls]
                dominant_edge = edge_names[np.argmax(weights)]
                print(f"  {class_name:<15} {weights[0]:<12.3f} {weights[1]:<12.3f} {weights[2]:<12.3f} {dominant_edge:<12}")
        
        # Classification report
        print("\n" + "="*70)
        print("üìã Detailed Classification Report:")
        print("="*70)
        class_names = config.get('class_names', [f'Class_{i}' for i in range(config['num_classes'])])
        print(classification_report(
            labels, preds,
            target_names=class_names,
            digits=4
        ))
        
        # Per-class F1 scores
        from sklearn.metrics import f1_score as sklearn_f1
        per_class_f1 = sklearn_f1(labels, preds, average=None, zero_division=0)
        print("\nüìä Per-Class F1 Scores:")
        print(f"  {'Class':<15} {'F1 Score':<10} {'Depth':<10} {'Dominant Edge':<15}")
        print(f"  {'-'*50}")
        for cls in range(config['num_classes']):
            class_name = class_names[cls] if cls < len(class_names) else f'Class_{cls}'
            f1 = per_class_f1[cls] if cls < len(per_class_f1) else 0.0
            depth = test_depth_per_class.get(cls, 0.0)
            
            if cls in test_edge_weights:
                edge_names = ['Same_Src', 'Same_Dst', 'Bidir']
                dominant_edge = edge_names[np.argmax(test_edge_weights[cls])]
            else:
                dominant_edge = 'N/A'
            
            print(f"  {class_name:<15} {f1:<10.4f} {depth:<10.2f} {dominant_edge:<15}")
        
        # Save detailed results
        results = {
            'test_metrics': {
                'loss': test_loss,
                'accuracy': test_acc,
                'f1_macro': test_f1_macro,
                'f1_weighted': test_f1_weighted
            },
            'per_class_f1': per_class_f1.tolist(),
            'depth_per_class': test_depth_per_class,
            'edge_weights_per_class': {k: v.tolist() for k, v in test_edge_weights.items()},
            'predictions': preds,
            'labels': labels
        }
        
        import json
        with open('test_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nüíæ Detailed results saved to 'test_results.json'")
    
    cleanup_ddp()


def main():
    """Main training function"""
    
    # Configuration
    config = {
        # Model architecture
        'num_experts': 6,
        'experts_per_sample': 2,
        'hidden_channels': 256,
        'num_layers': 3,
        'num_heads': 4,
        'dropout': 0.3,
        
        # Training
        'batch_size': 8192,  # Large batch for 8 GPUs
        'learning_rate': 0.001,
        'weight_decay': 1e-4,
        'epochs': 100,
        'patience': 15,
        'num_classes': 10,
        
        # Dynamic components
        'lambda_depth': 0.01,  # Depth penalty
        'lambda_load_balance': 0.01,  # Load balance penalty
        'warmup_epochs': 10,  # Warmup without dynamics
        'enable_dynamics_epoch': 30,  # When to enable dynamics
        
        # Class names for better reporting
        'class_names': [
            'Benign',
            'Scanning', 
            'XSS',
            'DDoS',
            'Password',
            'DOS',
            'Injection',
            'Backdoor',
            'MITM',
            'Ransomware'
        ]
    }
    
    # Define feature columns - IMPORTANT: Update this with your actual features
    # Based on your data analysis, here are recommended features:
    feature_cols = [
        'L4_SRC_PORT',
        'L4_DST_PORT',
        'PROTOCOL',
        'L7_PROTO',
        'IN_BYTES',
        'IN_PKTS',
        'OUT_BYTES',
        'OUT_PKTS',
        'TCP_FLAGS',
        'CLIENT_TCP_FLAGS',
        'SERVER_TCP_FLAGS',
        'FLOW_DURATION_MILLISECONDS',
        'DURATION_IN',
        'DURATION_OUT',
        'MIN_TTL',
        'MAX_TTL',
        'LONGEST_FLOW_PKT',
        'SHORTEST_FLOW_PKT',
        'MIN_IP_PKT_LEN',
        'MAX_IP_PKT_LEN',
        'RETRANSMITTED_IN_BYTES',
        'RETRANSMITTED_IN_PKTS',
        'RETRANSMITTED_OUT_BYTES',
        'RETRANSMITTED_OUT_PKTS',
        'SRC_TO_DST_AVG_THROUGHPUT',
        'DST_TO_SRC_AVG_THROUGHPUT',
        'NUM_PKTS_UP_TO_128_BYTES',
        'NUM_PKTS_128_TO_256_BYTES',
        'NUM_PKTS_256_TO_512_BYTES',
        'NUM_PKTS_512_TO_1024_BYTES',
        'NUM_PKTS_1024_TO_1514_BYTES',
        'TCP_WIN_MAX_IN',
        'TCP_WIN_MAX_OUT',
        'ICMP_TYPE',
        'ICMP_IPV4_TYPE',
        'DNS_QUERY_ID',
        'DNS_QUERY_TYPE',
        'DNS_TTL_ANSWER',
        'FTP_COMMAND_RET_CODE'
    ]
    
    print("="*70)
    print("üöÄ Dynamic Heterogeneous Graph Transformer")
    print("   Network Intrusion Detection System")
    print("="*70)
    print(f"\nüìù Configuration:")
    print(f"  Experts: {config['num_experts']}")
    print(f"  Experts per sample: {config['experts_per_sample']}")
    print(f"  Hidden channels: {config['hidden_channels']}")
    print(f"  Layers: {config['num_layers']} (dynamic depth)")
    print(f"  Attention heads: {config['num_heads']}")
    print(f"  Batch size: {config['batch_size']}")
    print(f"  Features: {len(feature_cols)}")
    
    # Load data
    data = load_and_prepare_data(feature_cols)
    
    # Get number of available GPUs
    world_size = torch.cuda.device_count()
    print(f"\n{'='*70}")
    print(f"üíª Hardware: {world_size} GPU(s) detected")
    print(f"{'='*70}\n")
    
    if world_size > 1:
        # Multi-GPU training with DDP
        print(f"üî• Starting distributed training on {world_size} GPUs...\n")
        mp.spawn(
            train_worker,
            args=(world_size, data, feature_cols, config),
            nprocs=world_size,
            join=True
        )
    else:
        # Single GPU training
        print(f"üî• Starting training on 1 GPU...\n")
        train_worker(0, 1, data, feature_cols, config)
    
    print("\n" + "="*70)
    print("‚úÖ Training completed!")
    print("="*70)


if __name__ == "__main__":
    main()
```

---

## **Key Features Implemented:**

### **1. Dynamic Depth (Adaptive Computation)**
- ‚úÖ Halting units at each layer
- ‚úÖ Class-aware depth penalty (no penalty for rare classes)
- ‚úÖ Tracks average depth per class
- ‚úÖ Expected speedup calculation

### **2. Edge Type Selection**
- ‚úÖ Learned edge weights per sample
- ‚úÖ Weighted message aggregation
- ‚úÖ Tracks dominant edge type per class
- ‚úÖ Interpretable edge patterns

### **3. Mixture of Experts**
- ‚úÖ 6 experts with protocol router
- ‚úÖ Top-2 sparse routing
- ‚úÖ Load balancing loss
- ‚úÖ Expert usage tracking

### **4. Phased Training**
- ‚úÖ Warmup phase (epochs 0-10): No dynamics
- ‚úÖ Enable dynamics (epochs 10-30): Gradual activation
- ‚úÖ Full training (epochs 30-100): All dynamics active

### **5. Comprehensive Monitoring**
- ‚úÖ Depth statistics per class
- ‚úÖ Edge weight analysis per class
- ‚úÖ Expert specialization tracking
- ‚úÖ Speedup estimation
- ‚úÖ Per-class F1 scores

---

## **Expected Output Example:**
```
üìä Validation Metrics:
  Loss: 0.1234
  Accuracy: 0.9567
  F1 (Macro): 0.8934
  F1 (Weighted): 0.9512

üìà Average Depth per Class (Validation):
  Benign: 1.23 layers        ‚Üê Fast exit!
  Scanning: 1.45 layers      ‚Üê Fast exit!
  XSS: 2.12 layers
  DDoS: 2.34 layers
  Password: 2.56 layers
  DOS: 2.41 layers
  Injection: 2.78 layers
  Backdoor: 3.00 layers      ‚Üê Full depth!
  MITM: 3.00 layers          ‚Üê Full depth!
  Ransomware: 3.00 layers    ‚Üê Full depth!

üîó Edge Weights per Class (Validation):
  Class           Same_Src     Same_Dst     Bidir
  ---------------------------------------------------
  Benign          0.333        0.334        0.333
  Scanning        0.712        0.145        0.143  ‚Üê Same_Src dominant!
  XSS             0.456        0.289        0.255
  DDoS            0.123        0.789        0.088  ‚Üê Same_Dst dominant!
  Password        0.567        0.234        0.199
  DOS             0.445        0.378        0.177
  Injection       0.489        0.312        0.199
  Backdoor        0.134        0.156        0.710  ‚Üê Bidirectional!
  MITM            0.145        0.167        0.688  ‚Üê Bidirectional!
  Ransomware      0.123        0.134        0.743  ‚Üê Bidirectional!

ü§ñ Expert Usage (Training):
  Expert 0: 18.2%  ‚Üê HTTP/HTTPS traffic
  Expert 1: 16.8%  ‚Üê DNS traffic
  Expert 2: 15.4%  ‚Üê Mixed protocols
  Expert 3: 19.3%  ‚Üê Attack patterns
  Expert 4: 14.7%  ‚Üê Rare attacks (ransomware)
  Expert 5: 15.6%  ‚Üê Generic

‚ö° Performance Gain:
  Average Depth: 1.89 / 3 layers
  Expected Speedup: 1.59x
