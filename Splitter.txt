from langchain_community.llms import VLLMOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# Define the endpoint for your vLLM server
VLLM_API_BASE = "http://localhost:8000/v1"

# Instantiate the VLLM model
llm = VLLMOpenAI(
    openai_api_base=VLLM_API_BASE,
    openai_api_key="EMPTY",  # vLLM does not require an API key
    model_name="Qwen/Qwen2.5-72B-Instruction-AWQ",
    temperature=0.7,
    max_tokens=1024,
)

# You can now use this 'llm' object in all your LangChain chains and agents.
# Example: a simple prompt template
prompt = PromptTemplate.from_template("Question: {question}\nAnswer: Let's think step by step.")

chain = prompt | llm | StrOutputParser()

# Invoke the chain
question = "What is the capital of France?"
response = chain.invoke({"question": question})
print(response)

# Example with a simple RAG chain (Retrieval-Augmented Generation)
# (Assuming you have a retriever set up)
# from langchain_community.vectorstores import FAISS
# from langchain_community.embeddings import HuggingFaceEmbeddings
# from langchain.chains import RetrievalQA

# # A dummy retriever for illustration
# class DummyRetriever:
#     def get_relevant_documents(self, query):
#         return ["France is a country in Western Europe. Its capital is Paris."]

# rag_chain = (
#     {"context": RunnablePassthrough(), "question": RunnablePassthrough()}
#     | prompt
#     | llm
#     | StrOutputParser()
# )
# print(rag_chain.invoke({"question": "What is the capital of France?", "context": "Paris is the capital of France."}))
