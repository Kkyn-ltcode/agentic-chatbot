import os
import asyncio
from operator import itemgetter
from typing import List, Optional, Any
from functools import partial

# --- Agent and Tool Specific Imports ---
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.agents import AgentExecutor, create_react_agent
from langchain.tools import Tool, tool
from langchain.chains import LLMMathChain

# --- Existing RAG and Memory Imports ---
from transformers import AutoTokenizer

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationSummaryBufferMemory
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage

# --- Custom LLM Class with Qwen Token Counter (UNCHANGED) ---
class QwenTokenCountingChatOpenAI(ChatOpenAI):
    tokenizer: AutoTokenizer = None

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        if self.tokenizer is None:
            # Use the exact tokenizer name you are using for your Qwen model
            tokenizer_name_or_path = "Qwen/Qwen1.5-72B-Chat-AWQ" # !!! ADJUST THIS if needed !!!
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)
            except Exception as e:
                print(f"ERROR: Could not load Qwen tokenizer from '{tokenizer_name_or_path}': {e}")
                # Fallback to a generic tokenizer if Qwen's isn't found
                self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        
        # Ensure chat_template is set for Qwen models
        if not hasattr(self.tokenizer, 'apply_chat_template') or self.tokenizer.chat_template is None:
            self.tokenizer.chat_template = (
                "{% for message in messages %}"
                "{% if message['role'] == 'user' %}"
                "{{ '<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% elif message['role'] == 'assistant' %}"
                "{{ '<|im_start|>assistant\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% elif message['role'] == 'system' %}"
                "{{ '<|im_start|>system\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% else %}"
                "{{ '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% endif %}"
                "{% endfor %}"
                "{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}"
            )

    def _get_num_tokens_from_messages(self, messages: List[BaseMessage]) -> int:
        if self.tokenizer is None:
            # Fallback for token counting if tokenizer fails
            return sum(len(msg.content.split()) for msg in messages)

        hf_messages = []
        for msg in messages:
            if isinstance(msg, HumanMessage):
                hf_messages.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                hf_messages.append({"role": "assistant", "content": msg.content})
            elif isinstance(msg, SystemMessage):
                hf_messages.append({"role": "system", "content": msg.content})
            else:
                hf_messages.append({"role": msg.type, "content": msg.content})

        try:
            token_ids = self.tokenizer.apply_chat_template(
                hf_messages,
                tokenize=True,
                add_generation_prompt=True
            )
            return len(token_ids)
        except Exception as e:
            print(f"Warning: Failed to apply chat template for token counting: {e}. Falling back to word count.")
            return sum(len(msg.content.split()) for msg in messages)


# --- Helper Function for Formatting Retrieved Documents (UNCHANGED) ---
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


# --- 1. RAG Setup: Embeddings, Vector Store, and Retriever (UNCHANGED) ---
print("Setting up RAG components...")
embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en-v1.5")
doc_vectorstore = FAISS.from_documents(
    [
        Document(page_content="The Amazon rainforest is located in South America."),
        Document(page_content="The Amazon river is the largest river by discharge volume."),
        Document(page_content="The sun is a star. It is in the center of the solar system."),
        Document(page_content="The Earth orbits the sun in an elliptical path."),
        Document(page_content="Photosynthesis is the process by which plants convert light energy into chemical energy."),
        Document(page_content="The capital of France is Paris."),
        Document(page_content="The Eiffel Tower is located in Paris."),
        Document(page_content="Mount Everest is the highest mountain in the world.")
    ],
    embeddings
)
retriever = doc_vectorstore.as_retriever(search_kwargs={"k": 2})
print("RAG components ready.")


# --- 2. LLM Initialization (UNCHANGED) ---
print("Initializing LLM...")
llm = QwenTokenCountingChatOpenAI(
    model="qwen-72b-awq", # !!! ADJUST THIS if needed !!!
    openai_api_base="http://localhost:8000/v1",
    openai_api_key="EMPTY",
    max_tokens=2048,
    temperature=0.7,
    stop=["<|im_end|>", "----", "Human:", "\nHuman:", "\n\nHuman:"]
)
print("LLM ready.")


# --- 3. Memory Setup (In-Memory Summary Buffer) (UNCHANGED) ---
print("Setting up memory manager (in-memory)...")
memory_manager = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=1024,
    return_messages=True, # Important for agent history
    memory_key="chat_history" # Key for the history in the agent prompt
)
print("Memory manager ready.")


# --- 4. Define Tools for the Agent ---
print("Defining tools for the agent...")

# Tool 1: Web Search
search_tool = DuckDuckGoSearchRun(
    name="web_search",
    description="Useful for answering questions about current events or general knowledge that is NOT in the provided knowledge base."
)

# Tool 2: Calculator
math_chain = LLMMathChain.from_llm(llm=llm, verbose=False)
calculator_tool = Tool(
    name="Calculator",
    func=math_chain.run,
    description="Useful for when you need to answer questions about math or perform calculations."
)

# Tool 3: Your RAG Knowledge Base as a Tool
# We need to wrap the RAG chain into a tool.
# The RAG chain needs access to history and current question to generate context.
# We'll create a function that replicates the RAG chain's core logic.
# Use @tool decorator for simpler tool definition
@tool
def knowledge_base_search(query: str) -> str:
    """
    Useful for answering questions about specific predefined knowledge,
    such as facts about the Amazon rainforest, the sun, or geographical data
    that might be in the bot's internal knowledge base.
    Always prioritize using this tool for factual questions that seem
    like they should be in the bot's existing documents.
    """
    # Load history from memory, as the RAG chain might need it for context
    history_messages = memory_manager.load_memory_variables({})["chat_history"]

    # Reconstruct a simplified RAG chain for the tool's purpose
    # This chain will only focus on context retrieval and then LLM generation
    rag_tool_chain = (
        RunnablePassthrough.assign(
            context=itemgetter("question") | retriever | format_docs,
            history=RunnableLambda(lambda x: history_messages) # Pass the loaded history
        )
        | ChatPromptTemplate.from_messages([
            ("system", "You are a helpful AI assistant. Use the following retrieved context to answer the question. "
                       "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n"
                       "Retrieved Context:\n{context}\n\n"
                       "Chat History:\n{history}"), # Include history in the RAG tool's prompt
            ("human", "{question}")
        ])
        | llm
        | StrOutputParser()
    )
    
    # Invoke the RAG tool chain with the query
    result = rag_tool_chain.invoke({"question": query})
    return result

tools = [search_tool, calculator_tool, knowledge_base_search]
print("Tools defined:", [t.name for t in tools])


# --- 5. Define the Agent Prompt ---
# This is the crucial part that sets up the ReAct prompt for the agent.
# It includes placeholders for tools, tool names, chat history, and the agent's scratchpad.
agent_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI assistant. You have access to the following tools: {tools}\n"
               "You should only use the tools if necessary and for factual information. "
               "Prioritize using 'knowledge_base_search' for facts that seem to be in a static knowledge base, "
               "and 'web_search' for current events or general knowledge.\n"
               "\n"
               "Follow this exact format:\n\n"
               "Question: the input question you must answer\n"
               "Thought: you should always think about what to do\n"
               "Action: the action to take, should be one of [{tool_names}]\n"
               "Action Input: the input to the action\n"
               "Observation: the result of the action\n"
               "...\n" # This "..." indicates potential multiple Thought/Action/Observation cycles
               "Thought: I now know the final answer\n"
               "Final Answer: the final answer to the original input question"),
    MessagesPlaceholder(variable_name="chat_history"), # Placeholder for past conversation history
    ("human", "{input}"), # Placeholder for the current user's question
    MessagesPlaceholder(variable_name="agent_scratchpad") # This is where the agent's thoughts/actions go
])
print("Agent prompt template ready.")


# --- 6. Create the Agent and Executor ---
print("Creating agent and executor...")
agent = create_react_agent(llm=llm, tools=tools, prompt=agent_prompt)

agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True, # Set to True to see the agent's thought process
    handle_parsing_errors=True # Crucial for robustness if LLM output isn't perfect
)
print("Agent executor ready.")


# --- 7. Main Asynchronous Execution Loop ---
async def main():
    print("\nWelcome to the RAG Chatbot with Agents and In-Memory Summary! Type 'exit' to quit.")
    print("This memory will reset each time the script is restarted.\n")

    while True:
        question = input("You: ")
        if question.lower() == 'exit':
            break

        print("Bot: ", end="", flush=True)
        
        # Load chat history from memory before invoking the agent
        # The agent_executor needs the chat_history explicitly
        history_for_agent = memory_manager.load_memory_variables({})["chat_history"]

        full_response_content = ""
        try:
            # Astream from agent_executor. It yields different types of chunks.
            async for chunk in agent_executor.astream(
                {"input": question, "chat_history": history_for_agent}
            ):
                # The agent_executor.astream() yields dictionaries.
                # We are interested in the 'output' key for the final answer.
                if "output" in chunk:
                    print(chunk["output"], end="", flush=True)
                    full_response_content += chunk["output"]
                # You can add more logic here to display thoughts/actions if verbose=False
                # For verbose=True, LangChain's AgentExecutor already prints step-by-step.
            print() # Add a newline after the streamed response

            # Save the current turn to the memory manager
            memory_manager.save_context(
                {"input": question},
                {"output": full_response_content}
            )

        except Exception as e:
            print(f"\nAn error occurred: {e}")
            full_response_content = "I apologize, but I encountered an error trying to process that request."
            print(full_response_content)
            # Still save context to memory even if an error occurred
            memory_manager.save_context(
                {"input": question},
                {"output": full_response_content}
            )

if __name__ == "__main__":
    asyncio.run(main())
