import torch

# Load your raw features
x = ...  # Your (16M, 43) tensor

print("Original stats:")
print(f"Min: {x.min():.2e}, Max: {x.max():.2e}, Median: {x.median():.2e}")

# STEP 1: Remove catastrophic outliers (clip to reasonable float range)
print("\nClipping catastrophic values...")
x = torch.clamp(x, min=0, max=1e10)  # Cap at 10 billion

print("After clipping:")
print(f"Min: {x.min():.2e}, Max: {x.max():.2e}, Median: {x.median():.2e}")
print(f"Mean: {x.mean():.2e}, Std: {x.std():.2e}")

# STEP 2: Now run the analysis again
print("\n" + "="*60)
print("RE-ANALYZING AFTER CLIPPING:")
print("="*60)

# Re-run per-feature analysis
for i in range(x.shape[1]):
    feat = x[:, i]
    zeros_pct = (feat == 0).sum().item() / feat.shape[0] * 100
    
    print(f"Feat {i}: min={feat.min():.2e}, max={feat.max():.2e}, "
          f"mean={feat.mean():.2e}, std={feat.std():.2e}, zeros={zeros_pct:.1f}%")
