import torch
import numpy as np

class NetworkFlowNormalizer:
    """
    Robust normalization for network flow data with extreme outliers.
    Handles zeros, skewed distributions, and values ranging to 1e+304.
    """
    
    def __init__(self, method='log_standard', clip_percentile=99.9):
        """
        Args:
            method: 'log_standard', 'robust', 'clip_standard', or 'minmax_log'
            clip_percentile: percentile to clip at (e.g., 99.9)
        """
        self.method = method
        self.clip_percentile = clip_percentile
        self.stats = {}
        
    def fit(self, x, sample_size=500000):
        """
        Compute statistics needed for normalization.
        
        Args:
            x: torch.Tensor of shape (N, 43)
            sample_size: number of rows to sample for percentile computation
        """
        print(f"Computing normalization statistics using method: {self.method}")
        n_rows, n_features = x.shape
        
        # Sample for percentile computation (memory efficient)
        if n_rows > sample_size:
            indices = torch.randperm(n_rows)[:sample_size]
            x_sample = x[indices]
        else:
            x_sample = x
        
        x_sample_np = x_sample.cpu().numpy()
        
        for i in range(n_features):
            feat = x[:, i]
            feat_sample = x_sample_np[:, i]
            
            # Store per-feature stats
            self.stats[i] = {}
            
            if self.method == 'log_standard':
                # Compute stats on log-transformed non-zero values
                feat_nonzero = feat[feat > 0]
                if len(feat_nonzero) > 0:
                    log_feat = torch.log1p(feat_nonzero)  # log(1 + x)
                    self.stats[i]['mean'] = log_feat.mean().item()
                    self.stats[i]['std'] = log_feat.std().item()
                else:
                    self.stats[i]['mean'] = 0.0
                    self.stats[i]['std'] = 1.0
                    
            elif self.method == 'robust':
                # Use median and IQR (immune to outliers)
                feat_sample_nonzero = feat_sample[feat_sample > 0]
                if len(feat_sample_nonzero) > 0:
                    self.stats[i]['median'] = np.median(feat_sample_nonzero)
                    self.stats[i]['q25'] = np.percentile(feat_sample_nonzero, 25)
                    self.stats[i]['q75'] = np.percentile(feat_sample_nonzero, 75)
                    self.stats[i]['iqr'] = self.stats[i]['q75'] - self.stats[i]['q25']
                    if self.stats[i]['iqr'] < 1e-6:
                        self.stats[i]['iqr'] = 1.0
                else:
                    self.stats[i]['median'] = 0.0
                    self.stats[i]['iqr'] = 1.0
                    
            elif self.method == 'clip_standard':
                # Clip at percentile, then standardize
                if len(feat_sample[feat_sample > 0]) > 0:
                    clip_val = np.percentile(feat_sample, self.clip_percentile)
                    self.stats[i]['clip_val'] = clip_val
                    
                    # Compute mean/std on clipped full data
                    feat_clipped = torch.clamp(feat, max=clip_val)
                    self.stats[i]['mean'] = feat_clipped.mean().item()
                    self.stats[i]['std'] = feat_clipped.std().item()
                else:
                    self.stats[i]['clip_val'] = 1.0
                    self.stats[i]['mean'] = 0.0
                    self.stats[i]['std'] = 1.0
                    
            elif self.method == 'minmax_log':
                # Log transform then scale to [0, 1]
                feat_nonzero = feat[feat > 0]
                if len(feat_nonzero) > 0:
                    log_feat = torch.log1p(feat_nonzero)
                    self.stats[i]['log_min'] = log_feat.min().item()
                    self.stats[i]['log_max'] = log_feat.max().item()
                    self.stats[i]['log_range'] = self.stats[i]['log_max'] - self.stats[i]['log_min']
                    if self.stats[i]['log_range'] < 1e-6:
                        self.stats[i]['log_range'] = 1.0
                else:
                    self.stats[i]['log_min'] = 0.0
                    self.stats[i]['log_max'] = 1.0
                    self.stats[i]['log_range'] = 1.0
        
        print(f"âœ“ Fitted on {n_features} features")
        return self
    
    def transform(self, x):
        """
        Apply normalization to data.
        
        Args:
            x: torch.Tensor of shape (N, 43)
            
        Returns:
            Normalized tensor of same shape
        """
        x_norm = torch.zeros_like(x, dtype=torch.float32)
        n_features = x.shape[1]
        
        for i in range(n_features):
            feat = x[:, i]
            
            if self.method == 'log_standard':
                # log(1 + x), then standardize
                feat_log = torch.log1p(feat)
                mean = self.stats[i]['mean']
                std = self.stats[i]['std']
                if std > 1e-6:
                    x_norm[:, i] = (feat_log - mean) / std
                else:
                    x_norm[:, i] = feat_log - mean
                    
            elif self.method == 'robust':
                # (x - median) / IQR
                median = self.stats[i]['median']
                iqr = self.stats[i]['iqr']
                x_norm[:, i] = (feat - median) / iqr
                
            elif self.method == 'clip_standard':
                # Clip then standardize
                clip_val = self.stats[i]['clip_val']
                feat_clipped = torch.clamp(feat, max=clip_val)
                mean = self.stats[i]['mean']
                std = self.stats[i]['std']
                if std > 1e-6:
                    x_norm[:, i] = (feat_clipped - mean) / std
                else:
                    x_norm[:, i] = feat_clipped - mean
                    
            elif self.method == 'minmax_log':
                # Log then scale to [0, 1]
                feat_log = torch.log1p(feat)
                log_min = self.stats[i]['log_min']
                log_range = self.stats[i]['log_range']
                x_norm[:, i] = (feat_log - log_min) / log_range
        
        return x_norm
    
    def fit_transform(self, x, sample_size=500000):
        """Fit and transform in one step."""
        self.fit(x, sample_size)
        return self.transform(x)


# RECOMMENDED USAGE FOR NETWORK FLOW DATA
print("=" * 80)
print("RECOMMENDED APPROACH FOR NETWORK FLOW DATA")
print("=" * 80)
print("""
Network flow data (bytes, packets, duration, etc.) is typically:
- Right-skewed with many small values and few huge ones
- Contains many zeros (no traffic)
- Spans many orders of magnitude

BEST METHOD: 'log_standard'
- Handles zeros naturally with log1p (log(1 + x))
- Compresses extreme values
- Standard for network traffic analysis
""")

print("\n" + "=" * 80)
print("USAGE")
print("=" * 80)
print("""
# Option 1: Log + Standardization (RECOMMENDED for network flow)
normalizer = NetworkFlowNormalizer(method='log_standard')
x_normalized = normalizer.fit_transform(x)

# Option 2: Robust scaling (if you want to preserve outliers)
normalizer = NetworkFlowNormalizer(method='robust')
x_normalized = normalizer.fit_transform(x)

# Option 3: Clip extreme outliers then standardize
normalizer = NetworkFlowNormalizer(method='clip_standard', clip_percentile=99.9)
x_normalized = normalizer.fit_transform(x)

# Option 4: Log + MinMax [0,1] scaling
normalizer = NetworkFlowNormalizer(method='minmax_log')
x_normalized = normalizer.fit_transform(x)

# Then use x_normalized in your graph model
""")

print("\n" + "=" * 80)
print("VERIFICATION")
print("=" * 80)
print("""
After normalization, check the result:

x_norm = normalizer.fit_transform(x)
print(f"Normalized - Min: {x_norm.min().item():.6f}")
print(f"Normalized - Max: {x_norm.max().item():.6f}")
print(f"Normalized - Mean: {x_norm.mean().item():.6f}")
print(f"Normalized - Std: {x_norm.std().item():.6f}")
print(f"NaN count: {torch.isnan(x_norm).sum().item()}")
print(f"Inf count: {torch.isinf(x_norm).sum().item()}")
""")
