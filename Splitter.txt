import os
import asyncio
from operator import itemgetter
from typing import List, Optional, Any, TypedDict, Union, Annotated
from functools import partial

# --- LangGraph Imports ---
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolExecutor, ToolInvocation

# --- Agent and Tool Specific Imports ---
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.tools import Tool, tool
from langchain.chains import LLMMathChain

# --- Existing RAG and Memory Imports ---
from transformers import AutoTokenizer

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationSummaryBufferMemory
from langchain_core.documents import Document
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage

# --- Imports for manual agent construction ---
from langchain.agents.format_scratchpad import format_log_to_messages
from langchain.agents.output_parsers import ReActSingleInputOutputParser
from langchain_core.agents import AgentAction, AgentFinish


# --- Custom LLM Class with Qwen Token Counter ---
class QwenTokenCountingChatOpenAI(ChatOpenAI):
    tokenizer: AutoTokenizer = None

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        if self.tokenizer is None:
            tokenizer_name_or_path = "Qwen/Qwen1.5-72B-Chat-AWQ"
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)
            except Exception as e:
                print(f"ERROR: Could not load Qwen tokenizer from '{tokenizer_name_or_path}': {e}")
                self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        
        if not hasattr(self.tokenizer, 'apply_chat_template') or self.tokenizer.chat_template is None:
            self.tokenizer.chat_template = (
                "{% for message in messages %}"
                "{% if message['role'] == 'user' %}"
                "{{ '<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% elif message['role'] == 'assistant' %}"
                "{{ '<|im_start|>assistant\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% elif message['role'] == 'system' %}"
                "{{ '<|im_start|>system\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% else %}"
                "{{ '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}"
                "{% endif %}"
                "{% endfor %}"
                "{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}"
            )

    def _get_num_tokens_from_messages(self, messages: List[BaseMessage]) -> int:
        if self.tokenizer is None:
            return sum(len(msg.content.split()) for msg in messages)

        hf_messages = []
        for msg in messages:
            if isinstance(msg, HumanMessage):
                hf_messages.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                hf_messages.append({"role": "assistant", "content": msg.content})
            elif isinstance(msg, SystemMessage):
                hf_messages.append({"role": "system", "content": msg.content})
            else:
                hf_messages.append({"role": msg.type, "content": msg.content})

        try:
            token_ids = self.tokenizer.apply_chat_template(
                hf_messages,
                tokenize=True,
                add_generation_prompt=True
            )
            return len(token_ids)
        except Exception as e:
            print(f"Warning: Failed to apply chat template for token counting: {e}. Falling back to word count.")
            return sum(len(msg.content.split()) for msg in messages)


# --- Helper Function for Formatting Retrieved Documents ---
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


# --- 1. RAG Setup: Embeddings, Vector Store, and Retriever ---
print("Setting up RAG components...")
embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en-v1.5")
doc_vectorstore = FAISS.from_documents(
    [
        Document(page_content="The Amazon rainforest is located in South America."),
        Document(page_content="The Amazon river is the largest river by discharge volume."),
        Document(page_content="The sun is a star. It is in the center of the solar system."),
        Document(page_content="The Earth orbits the sun in an elliptical path."),
        Document(page_content="Photosynthesis is the process by which plants convert light energy into chemical energy."),
        Document(page_content="The capital of France is Paris."),
        Document(page_content="The Eiffel Tower is located in Paris."),
        Document(page_content="Mount Everest is the highest mountain in the world.")
    ],
    embeddings
)
retriever = doc_vectorstore.as_retriever(search_kwargs={"k": 2})
print("RAG components ready.")


# --- 2. LLM Initialization ---
print("Initializing LLM...")
llm = QwenTokenCountingChatOpenAI(
    model="qwen-72b-awq",
    openai_api_base="http://localhost:8000/v1",
    openai_api_key="EMPTY",
    max_tokens=2048,
    temperature=0.0,
    stop=["<|im_end|>", "----", "Human:", "\nHuman:", "\n\nHuman:"]
)
print("LLM ready.")


# --- 3. Memory Setup (In-Memory Summary Buffer) ---
print("Setting up memory manager (in-memory)...")
# Note: Memory will now be managed by LangGraph state and explicitly updated.
# This ConversationSummaryBufferMemory is still useful for *summarizing* history,
# but the raw chat_history will be in the graph state.
memory_manager = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=1024,
    return_messages=True,
    memory_key="chat_history"
)
print("Memory manager ready.")


# --- 4. Define Tools for the Agent ---
print("Defining tools for the agent...")

search_tool = DuckDuckGoSearchRun(
    name="web_search",
    description="Useful for answering questions about current events or general knowledge that is NOT in the provided knowledge base."
)

math_chain = LLMMathChain.from_llm(llm=llm, verbose=False)
calculator_tool = Tool(
    name="Calculator",
    func=math_chain.run,
    description="Useful for when you need to answer questions about math or perform calculations."
)

@tool
def knowledge_base_search(query: str) -> str:
    """
    Useful for answering questions about specific predefined knowledge,
    such as facts about the Amazon rainforest, the sun, or geographical data
    that might be in the bot's internal knowledge base.
    Always prioritize using this tool for factual questions that seem
    like they should be in the bot's existing documents.
    """
    # In LangGraph, chat_history will be part of the graph state, not directly loaded here.
    # However, for the internal RAG tool's prompt, we can still use the *overall* history from memory.
    history_messages = memory_manager.load_memory_variables({})["chat_history"]

    rag_tool_chain = (
        RunnablePassthrough.assign(
            context=itemgetter("question") | retriever | format_docs,
            history=RunnableLambda(lambda x: history_messages)
        )
        | ChatPromptTemplate.from_messages([
            ("system", "You are a helpful AI assistant. Use the following retrieved context to answer the question. "
                       "Provide a comprehensive and helpful answer based on the context. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n"
                       "Retrieved Context:\n{context}\n\n"
                       "Chat History:\n{history}"),
            ("human", "{question}")
        ])
        | llm
        | StrOutputParser()
    )
    
    result = rag_tool_chain.invoke({"question": query})
    return result

tools = [search_tool, calculator_tool, knowledge_base_search]
# LangGraph uses a ToolExecutor to run tools
tool_executor = ToolExecutor(tools)
print("Tools defined:", [t.name for t in tools])


# --- 5. Define the Agent Prompt (FINAL REFINEMENT) ---
# This prompt will be used within the LLM node of the graph.
agent_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI assistant. You have access to the following tools: {tools}\n"
               "You should only use the tools if necessary and for factual information. "
               "Prioritize using 'knowledge_base_search' for facts that seem to be in a static knowledge base, "
               "and 'web_search' for current events or general knowledge.\n"
               "\n"
               "**IMPORTANT INSTRUCTIONS:**\n"
               "Follow this exact format for your responses strictly and do not deviate. **You must generate only ONE logical step per turn.**\n\n"
               "Question: the input question you must answer\n"
               "Thought: you should always think about what to do\n"
               "Action: the action to take, should be one of [{tool_names}]\n"
               "Action Input: the input to the action\n"
               "Observation: the result of the action\n"
               "...\n"
               "Thought: I now know the final answer\n"
               "**Final Answer: <YOUR_FINAL_ANSWER_AS_A_HELPFUL_SENTENCE_GOES_HERE>**\n\n"
               "**FORMATTING INSTRUCTION:** Your final answer should always be a complete, clear, and helpful sentence that directly addresses the user's original question. Do not just output raw data, numbers, or single words unless the question specifically asks for only that.\n\n"
               "**CRITICAL:**\n"
               "1. If you decide to use a tool, your entire output for this turn MUST end immediately after 'Action Input: <tool_input>'.\n"
               "2. If you know the final answer, your entire output for this turn MUST end immediately after 'Final Answer: <final_answer>'. **This is your absolute final response and the agent will stop.**\n"
               "DO NOT generate 'Observation:' or any further 'Thought:' or 'Action:' after an 'Action Input:' unless it's a new turn after an Observation.\n"
               "DO NOT generate any content after 'Final Answer:'."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])
print("Agent prompt template ready.")


# --- LangGraph Specific Components ---

# 1. Define the Agent State
class AgentState(TypedDict):
    input: str
    chat_history: List[BaseMessage]
    agent_outcome: Union[AgentAction, AgentFinish, None] # Output of the LLM node
    intermediate_steps: Annotated[List[tuple[AgentAction, str]], itemgetter("intermediate_steps")] # List of (action, observation) tuples

# 2. Define Nodes

# Node for LLM invocation and parsing
def run_llm(state: AgentState):
    """
    Invokes the LLM with the current prompt and parses its output into AgentAction or AgentFinish.
    """
    print("\n--- NODE: run_llm ---")
    current_chat_history = state["chat_history"]
    current_input = state["input"]
    current_intermediate_steps = state["intermediate_steps"]

    # Format the intermediate steps into messages for the agent_scratchpad
    formatted_scratchpad = format_log_to_messages(current_intermediate_steps)

    # Prepare input for the LLM chain
    llm_input = {
        "input": current_input,
        "chat_history": current_chat_history,
        "agent_scratchpad": formatted_scratchpad,
        "tools": tools, # Tools are passed to the prompt formatter
        "tool_names": [t.name for t in tools] # Tool names are passed to the prompt formatter
    }

    # The LangChain Runnable for LLM invocation and parsing
    agent_runnable = (
        agent_prompt
        | llm.bind(stop=["\nObservation:"]) # Keep the crucial stop token here
        | ReActSingleInputOutputParser()
    )

    # Invoke the LLM
    agent_outcome = agent_runnable.invoke(llm_input)
    print(f"LLM Output (parsed): {agent_outcome}")

    # Return the updated state
    return {"agent_outcome": agent_outcome}

# Node for tool execution
async def execute_tool(state: AgentState):
    """
    Executes the tool specified by agent_outcome and updates intermediate_steps.
    """
    print("\n--- NODE: execute_tool ---")
    agent_action = state["agent_outcome"]
    tool_name = agent_action.tool
    tool_input = agent_action.tool_input

    print(f"Executing Tool: {tool_name} with Input: {tool_input}")

    # ToolInvocation is used by ToolExecutor
    invocation = ToolInvocation(tool=tool_name, tool_input=tool_input)
    
    try:
        observation = await tool_executor.ainvoke(invocation)
        print(f"Tool Observation: {observation}")
    except Exception as e:
        observation = f"Tool execution error: {e}"
        print(f"Tool Error: {observation}")

    # Append the (action, observation) to intermediate_steps
    new_intermediate_step = (agent_action, observation)
    
    # LangGraph state updates are typically immutable, so we create a new list
    updated_intermediate_steps = state["intermediate_steps"] + [new_intermediate_step]

    # Return the updated state
    return {"intermediate_steps": updated_intermediate_steps}


# 3. Define the Graph

# Conditional edge function: Decides the next step based on LLM's output
def decide_next_step(state: AgentState):
    """
    Decides whether the agent should continue by executing a tool or finish.
    """
    print("\n--- NODE: decide_next_step (Conditional) ---")
    agent_outcome = state["agent_outcome"]

    if isinstance(agent_outcome, AgentFinish):
        print("Decision: AgentFinish (END)")
        return END # Terminates the graph
    else:
        print("Decision: AgentAction (call_tool)")
        return "call_tool" # Transitions to the execute_tool node


# Build the LangGraph graph
workflow = StateGraph(AgentState)

# Add nodes to the graph
workflow.add_node("call_llm", run_llm)
workflow.add_node("call_tool", execute_tool)

# Set the entry point
workflow.set_entry_point("call_llm")

# Define edges
# From call_llm, go to decide_next_step (a conditional edge)
workflow.add_conditional_edges(
    "call_llm", # From this node
    decide_next_step, # Use this function to decide next node
    {
        "call_tool": "call_tool", # If decide_next_step returns "call_tool", go to call_tool node
        END: END # If decide_next_step returns END, terminate
    }
)

# From call_tool, always go back to call_llm (to get the next Thought/Action based on Observation)
workflow.add_edge("call_tool", "call_llm")

# Compile the graph
app = workflow.compile()

print("\nLangGraph application compiled.")


# --- 7. Main Asynchronous Execution Loop ---
async def main():
    print("\nWelcome to the RAG Chatbot with Agents and In-Memory Summary (LangGraph)! Type 'exit' to quit.")
    print("This memory will reset each time the script is restarted.\n")

    while True:
        question = input("You: ")
        if question.lower() == 'exit':
            break

        print("Bot: ", end="", flush=True)

        # Retrieve chat history from memory manager
        # LangGraph will manage intermediate_steps internally
        initial_chat_history = memory_manager.load_memory_variables({})["chat_history"]

        # Prepare initial state for the graph
        inputs = {
            "input": question,
            "chat_history": initial_chat_history,
            "intermediate_steps": [], # Start with empty intermediate steps
            "agent_outcome": None # Initial state has no agent outcome yet
        }

        full_response_content = ""
        try:
            # Iterate through the graph's execution steps
            async for s in app.astream(inputs):
                # LangGraph outputs the state after each node execution
                # The 'agent_outcome' will be populated when the LLM node runs
                if "agent_outcome" in s and s["agent_outcome"] is not None:
                    if isinstance(s["agent_outcome"], AgentFinish):
                        # This is the final answer, print it and break the stream
                        final_answer_message = s["agent_outcome"].return_values["output"]
                        print(final_answer_message, end="", flush=True)
                        full_response_content = final_answer_message
                        break # Exit the streaming loop as we have the final answer

                # For intermediate steps, print based on verbosity if needed
                # (The node print statements already provide good verbosity)

            print() # Newline after response

            # Save context to memory manager for summarization/future use
            memory_manager.save_context(
                {"input": question},
                {"output": full_response_content}
            )

        except Exception as e:
            print(f"\nAn error occurred: {e}")
            full_response_content = "I apologize, but I encountered an error trying to process that request."
            print(full_response_content)
            memory_manager.save_context(
                {"input": question},
                {"output": full_response_content}
            )

if __name__ == "__main__":
    asyncio.run(main())
