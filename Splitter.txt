import polars as pl
import multiprocessing as mp
from pathlib import Path
from itertools import product

# --------------------
# CONFIG
# --------------------
INPUT_PATH = "flows.parquet"  # dataset (ID, Src, Dst)
OUTPUT_PATH = "edges_global.parquet"
TMP_DIR = Path("edges_tmp")
TMP_DIR.mkdir(exist_ok=True)

# --------------------
# STEP 1: Load and prepare
# --------------------
print("ðŸ”¹ Loading dataset...")
df = pl.read_parquet(INPUT_PATH).select(["ID", "Src", "Dst"])

# Cast to categorical to optimize memory and hashing
df = df.with_columns([
    pl.col("Src").cast(pl.Categorical),
    pl.col("Dst").cast(pl.Categorical)
])

# --------------------
# STEP 2: Build global key â†’ list[ID] mappings
# --------------------
print("ðŸ”¹ Building global Src and Dst maps...")

src_map = (
    df.select(["Src", "ID"])
    .group_by("Src")
    .agg(pl.col("ID").alias("ids"))
)

dst_map = (
    df.select(["Dst", "ID"])
    .group_by("Dst")
    .agg(pl.col("ID").alias("ids"))
)

# Optional: Convert to Python dicts (for parallel processing)
src_dict = dict(zip(src_map["Src"], src_map["ids"]))
dst_dict = dict(zip(dst_map["Dst"], dst_map["ids"]))

print(f"âœ… Built {len(src_dict):,} Src keys and {len(dst_dict):,} Dst keys.")

# --------------------
# STEP 3: Parallel edge generation helper
# --------------------
def gen_edges_for_key(args):
    key, src_ids, dst_ids = args
    pairs = []

    # Case 1: Srcâ€“Src
    if src_ids and len(src_ids) > 1:
        pairs.extend(product(src_ids, src_ids))

    # Case 2: Dstâ€“Dst
    if dst_ids and len(dst_ids) > 1:
        pairs.extend(product(dst_ids, dst_ids))

    # Case 3 + 4: Srcâ€“Dst and Dstâ€“Src
    if src_ids and dst_ids:
        pairs.extend(product(src_ids, dst_ids))
        pairs.extend(product(dst_ids, src_ids))

    # Filter out self loops
    pairs = [(a, b) for (a, b) in pairs if a != b]
    return pairs

# --------------------
# STEP 4: Merge Src and Dst keyspaces
# --------------------
print("ðŸ”¹ Merging Src/Dst keys...")
all_keys = set(src_dict.keys()) | set(dst_dict.keys())

def key_args_generator():
    for key in all_keys:
        yield (
            key,
            src_dict.get(key, []),
            dst_dict.get(key, [])
        )

# --------------------
# STEP 5: Parallel generation across keys
# --------------------
print("ðŸ”¹ Generating all edges in parallel...")
with mp.Pool(processes=mp.cpu_count()) as pool:
    all_edges = pool.map(gen_edges_for_key, key_args_generator(), chunksize=500)

# Flatten and build Polars DataFrame
edges = pl.DataFrame(
    [pair for pairs in all_edges for pair in pairs],
    schema=["id1", "id2"]
).unique()

edges.write_parquet(OUTPUT_PATH)
print(f"âœ… Done. Global edges written to {OUTPUT_PATH}")
print(f"Total edges: {edges.shape[0]:,}")
