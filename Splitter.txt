import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import HeteroData
from torch_geometric.nn import Linear, HeteroConv, TransformerConv
from torch_geometric.loader import NeighborLoader
import polars as pl
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.distributed as dist
import torch.multiprocessing as mp
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score
import numpy as np
from tqdm import tqdm
import os
import warnings
warnings.filterwarnings('ignore')


class HeteroGraphTransformer(nn.Module):
    """Graph Transformer for Heterogeneous Graphs with multiple edge types"""
    
    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, 
                 num_heads=4, dropout=0.3, edge_types=None):
        super().__init__()
        
        self.num_layers = num_layers
        self.dropout = dropout
        
        # Input projection
        self.input_proj = Linear(in_channels, hidden_channels)
        
        # Graph Transformer layers with heterogeneous convolutions
        self.convs = nn.ModuleList()
        self.norms = nn.ModuleList()
        
        for _ in range(num_layers):
            conv_dict = {}
            for edge_type in edge_types:
                conv_dict[edge_type] = TransformerConv(
                    hidden_channels, 
                    hidden_channels // num_heads,
                    heads=num_heads,
                    dropout=dropout,
                    edge_dim=None,
                    beta=True  # Gated attention
                )
            
            self.convs.append(HeteroConv(conv_dict, aggr='sum'))
            self.norms.append(nn.LayerNorm(hidden_channels))
        
        # Output projection
        self.output_proj = nn.Sequential(
            Linear(hidden_channels, hidden_channels // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            Linear(hidden_channels // 2, out_channels)
        )
    
    def forward(self, x_dict, edge_index_dict):
        # Initial projection
        x_dict = {key: self.input_proj(x) for key, x in x_dict.items()}
        
        # Graph Transformer layers with residual connections
        for i, (conv, norm) in enumerate(zip(self.convs, self.norms)):
            x_dict_out = conv(x_dict, edge_index_dict)
            
            # Residual connection and normalization
            x_dict = {
                key: norm(F.dropout(x_out + x_dict[key], p=self.dropout, training=self.training))
                for key, x_out in x_dict_out.items()
            }
            
            # ReLU activation for hidden layers
            if i < self.num_layers - 1:
                x_dict = {key: F.relu(x) for key, x in x_dict.items()}
        
        # Output projection (only for 'flow' nodes)
        return self.output_proj(x_dict['flow'])


def load_and_prepare_data(feature_cols):
    """Load graph data and prepare train/val/test splits"""
    print("Loading data...")
    nodes = pl.read_parquet("graph/nodes_features.parquet")
    edges1 = pl.read_parquet("graph/edges_type1.parquet")
    edges2 = pl.read_parquet("graph/edges_type2.parquet")
    edges3 = pl.read_parquet("graph/edges_type3.parquet")
    
    # Create heterogeneous graph
    data = HeteroData()
    data['flow'].x = torch.tensor(nodes.select(feature_cols).to_numpy(), dtype=torch.float32)
    data['flow'].y = torch.tensor(nodes['label'].to_numpy(), dtype=torch.long)
    
    # Add edges
    data['flow', 'same_source', 'flow'].edge_index = torch.tensor(
        [edges1['u'].to_numpy(), edges1['v'].to_numpy()], dtype=torch.long
    )
    data['flow', 'same_dest', 'flow'].edge_index = torch.tensor(
        [edges2['u'].to_numpy(), edges2['v'].to_numpy()], dtype=torch.long
    )
    data['flow', 'bidirectional', 'flow'].edge_index = torch.tensor(
        [edges3['u'].to_numpy(), edges3['v'].to_numpy()], dtype=torch.long
    )
    
    # Create train/val/test masks
    num_nodes = data['flow'].x.size(0)
    indices = np.arange(num_nodes)
    
    # Stratified split
    train_idx, test_idx = train_test_split(
        indices, test_size=0.2, random_state=42, 
        stratify=nodes['label'].to_numpy()
    )
    train_idx, val_idx = train_test_split(
        train_idx, test_size=0.1, random_state=42,
        stratify=nodes['label'].to_numpy()[train_idx]
    )
    
    # Create masks
    data['flow'].train_mask = torch.zeros(num_nodes, dtype=torch.bool)
    data['flow'].val_mask = torch.zeros(num_nodes, dtype=torch.bool)
    data['flow'].test_mask = torch.zeros(num_nodes, dtype=torch.bool)
    
    data['flow'].train_mask[train_idx] = True
    data['flow'].val_mask[val_idx] = True
    data['flow'].test_mask[test_idx] = True
    
    print(f"Total nodes: {num_nodes:,}")
    print(f"Train: {len(train_idx):,}, Val: {len(val_idx):,}, Test: {len(test_idx):,}")
    print(f"Edge types: same_source={edges1.shape[0]:,}, same_dest={edges2.shape[0]:,}, bidirectional={edges3.shape[0]:,}")
    
    return data


def setup_ddp(rank, world_size):
    """Initialize distributed training"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)


def cleanup_ddp():
    """Cleanup distributed training"""
    dist.destroy_process_group()


def train_epoch(model, loader, optimizer, device, rank):
    """Train for one epoch"""
    model.train()
    total_loss = 0
    total_correct = 0
    total_samples = 0
    
    if rank == 0:
        loader = tqdm(loader, desc="Training")
    
    for batch in loader:
        batch = batch.to(device)
        optimizer.zero_grad()
        
        # Forward pass
        out = model(batch.x_dict, batch.edge_index_dict)
        
        # Only compute loss on training nodes in this batch
        loss = F.cross_entropy(out[batch['flow'].train_mask], 
                              batch['flow'].y[batch['flow'].train_mask])
        
        # Backward pass
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        
        # Metrics
        pred = out[batch['flow'].train_mask].argmax(dim=1)
        correct = (pred == batch['flow'].y[batch['flow'].train_mask]).sum().item()
        
        total_loss += loss.item() * batch['flow'].train_mask.sum().item()
        total_correct += correct
        total_samples += batch['flow'].train_mask.sum().item()
    
    return total_loss / total_samples, total_correct / total_samples


@torch.no_grad()
def evaluate(model, loader, device, rank, mask_name='val_mask'):
    """Evaluate model"""
    model.eval()
    total_loss = 0
    total_correct = 0
    total_samples = 0
    all_preds = []
    all_labels = []
    
    if rank == 0:
        loader = tqdm(loader, desc=f"Evaluating ({mask_name})")
    
    for batch in loader:
        batch = batch.to(device)
        
        # Forward pass
        out = model(batch.x_dict, batch.edge_index_dict)
        
        # Get mask
        mask = getattr(batch['flow'], mask_name)
        
        if mask.sum() > 0:
            loss = F.cross_entropy(out[mask], batch['flow'].y[mask])
            pred = out[mask].argmax(dim=1)
            
            total_loss += loss.item() * mask.sum().item()
            total_correct += (pred == batch['flow'].y[mask]).sum().item()
            total_samples += mask.sum().item()
            
            all_preds.extend(pred.cpu().numpy())
            all_labels.extend(batch['flow'].y[mask].cpu().numpy())
    
    avg_loss = total_loss / total_samples if total_samples > 0 else 0
    accuracy = total_correct / total_samples if total_samples > 0 else 0
    
    # Calculate F1 score
    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)
    f1_weighted = f1_score(all_labels, all_preds, average='weighted', zero_division=0)
    
    return avg_loss, accuracy, f1_macro, f1_weighted, all_preds, all_labels


def train_worker(rank, world_size, data, feature_cols, config):
    """Training worker for each GPU"""
    
    # Setup DDP
    setup_ddp(rank, world_size)
    device = torch.device(f'cuda:{rank}')
    
    # Create neighbor loaders for mini-batch training
    train_loader = NeighborLoader(
        data,
        num_neighbors=[15, 10, 5],  # 3-hop neighborhood sampling
        batch_size=config['batch_size'],
        input_nodes=('flow', data['flow'].train_mask),
        shuffle=True,
        num_workers=4,
        persistent_workers=True
    )
    
    val_loader = NeighborLoader(
        data,
        num_neighbors=[15, 10, 5],
        batch_size=config['batch_size'] * 2,
        input_nodes=('flow', data['flow'].val_mask),
        shuffle=False,
        num_workers=4,
        persistent_workers=True
    )
    
    # Initialize model
    edge_types = [
        ('flow', 'same_source', 'flow'),
        ('flow', 'same_dest', 'flow'),
        ('flow', 'bidirectional', 'flow')
    ]
    
    model = HeteroGraphTransformer(
        in_channels=len(feature_cols),
        hidden_channels=config['hidden_channels'],
        out_channels=config['num_classes'],
        num_layers=config['num_layers'],
        num_heads=config['num_heads'],
        dropout=config['dropout'],
        edge_types=edge_types
    ).to(device)
    
    # Wrap model with DDP
    model = DDP(model, device_ids=[rank], find_unused_parameters=False)
    
    # Optimizer and scheduler
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=config['learning_rate'],
        weight_decay=config['weight_decay']
    )
    
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, 
        T_max=config['epochs'],
        eta_min=1e-6
    )
    
    # Training loop
    best_val_f1 = 0
    patience_counter = 0
    
    for epoch in range(config['epochs']):
        if rank == 0:
            print(f"\n{'='*50}")
            print(f"Epoch {epoch + 1}/{config['epochs']}")
            print(f"{'='*50}")
        
        # Train
        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device, rank)
        
        # Validate
        val_loss, val_acc, val_f1_macro, val_f1_weighted, _, _ = evaluate(
            model, val_loader, device, rank, 'val_mask'
        )
        
        scheduler.step()
        
        if rank == 0:
            print(f"\nTrain Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
            print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
            print(f"Val F1 (Macro): {val_f1_macro:.4f}, Val F1 (Weighted): {val_f1_weighted:.4f}")
            print(f"Learning Rate: {scheduler.get_last_lr()[0]:.6f}")
            
            # Save best model
            if val_f1_weighted > best_val_f1:
                best_val_f1 = val_f1_weighted
                patience_counter = 0
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.module.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'val_f1': val_f1_weighted,
                }, 'best_model.pt')
                print(f"âœ“ Saved best model (F1: {best_val_f1:.4f})")
            else:
                patience_counter += 1
            
            # Early stopping
            if patience_counter >= config['patience']:
                print(f"\nEarly stopping triggered after {epoch + 1} epochs")
                break
    
    # Test evaluation on rank 0
    if rank == 0:
        print(f"\n{'='*50}")
        print("Final Evaluation on Test Set")
        print(f"{'='*50}")
        
        # Load best model
        checkpoint = torch.load('best_model.pt')
        model.module.load_state_dict(checkpoint['model_state_dict'])
        
        test_loader = NeighborLoader(
            data,
            num_neighbors=[15, 10, 5],
            batch_size=config['batch_size'] * 2,
            input_nodes=('flow', data['flow'].test_mask),
            shuffle=False,
            num_workers=4,
            persistent_workers=True
        )
        
        test_loss, test_acc, test_f1_macro, test_f1_weighted, preds, labels = evaluate(
            model, test_loader, device, rank, 'test_mask'
        )
        
        print(f"\nTest Loss: {test_loss:.4f}")
        print(f"Test Accuracy: {test_acc:.4f}")
        print(f"Test F1 (Macro): {test_f1_macro:.4f}")
        print(f"Test F1 (Weighted): {test_f1_weighted:.4f}")
        
        # Classification report
        print("\n" + "="*50)
        print("Classification Report:")
        print("="*50)
        print(classification_report(labels, preds, 
                                   target_names=[f'Class_{i}' for i in range(config['num_classes'])],
                                   digits=4))
    
    cleanup_ddp()


def main():
    """Main training function"""
    
    # Configuration
    config = {
        'batch_size': 4096,  # Large batch size for efficiency
        'hidden_channels': 256,
        'num_layers': 3,
        'num_heads': 4,
        'dropout': 0.3,
        'learning_rate': 0.001,
        'weight_decay': 1e-4,
        'epochs': 100,
        'patience': 15,
        'num_classes': 10,  # Normal + 9 attack types
    }
    
    # Define your feature columns
    feature_cols = [
        # Add your actual feature column names here
        # Example: 'duration', 'protocol', 'src_bytes', 'dst_bytes', etc.
    ]
    
    # Load data
    data = load_and_prepare_data(feature_cols)
    
    # Get number of available GPUs
    world_size = torch.cuda.device_count()
    print(f"\n{'='*50}")
    print(f"Training on {world_size} GPUs")
    print(f"{'='*50}\n")
    
    if world_size > 1:
        # Multi-GPU training with DDP
        mp.spawn(
            train_worker,
            args=(world_size, data, feature_cols, config),
            nprocs=world_size,
            join=True
        )
    else:
        # Single GPU training
        train_worker(0, 1, data, feature_cols, config)


if __name__ == "__main__":
    main()
