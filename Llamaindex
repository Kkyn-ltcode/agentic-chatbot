from docx import Document as DocxDocument
from nltk.tokenize import sent_tokenize
from transformers import AutoTokenizer
from langchain.schema import Document as LangChainDocument

# Load tokenizer for token-aware chunking
tokenizer = AutoTokenizer.from_pretrained("gpt2")  # Or your LLM tokenizer

# Step 1: Load .docx and parse paragraphs with style info
def load_docx_with_styles(path):
    docx = DocxDocument(path)
    sections = []
    current_section = {"title": "Untitled", "content": []}

    for para in docx.paragraphs:
        text = para.text.strip()
        if not text:
            continue

        style = para.style.name.lower()

        if "heading" in style:
            if current_section["content"]:
                sections.append(current_section)
            current_section = {"title": text, "content": []}
        else:
            current_section["content"].append(text)

    if current_section["content"]:
        sections.append(current_section)

    return sections

# Step 2: Sentence-aware, token-limited chunking
def chunk_sentences(sentences, max_tokens=512):
    chunks = []
    current_chunk = ""
    current_tokens = 0

    for sent in sentences:
        sent = sent.strip()
        if not sent:
            continue
        tokens = len(tokenizer.tokenize(sent))
        if current_tokens + tokens > max_tokens:
            chunks.append(current_chunk.strip())
            current_chunk = sent
            current_tokens = tokens
        else:
            current_chunk += " " + sent
            current_tokens += tokens

    if current_chunk.strip():
        chunks.append(current_chunk.strip())

    return chunks

# Step 3: Chunk each section by sentence
def chunk_section(section, max_tokens=512):
    all_sentences = []
    for para in section["content"]:
        all_sentences.extend(sent_tokenize(para))

    text_chunks = chunk_sentences(all_sentences, max_tokens)
    return [
        LangChainDocument(page_content=chunk, metadata={"section": section["title"]})
        for chunk in text_chunks
    ]

# Step 4: Full pipeline
def chunk_docx_to_documents(docx_path, max_tokens=512):
    sections = load_docx_with_styles(docx_path)
    all_chunks = []
    for section in sections:
        chunks = chunk_section(section, max_tokens)
        all_chunks.extend(chunks)
    return all_chunks
