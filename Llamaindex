import fitz  # PyMuPDF
from nltk.tokenize import sent_tokenize
from langchain.schema import Document as LangChainDocument
import tiktoken

# Tokenizer config
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

def count_tokens(text):
    return len(encoding.encode(text))

def chunk_sentences(sentences, max_tokens=512):
    chunks = []
    current_chunk = ""
    current_tokens = 0

    for sent in sentences:
        tokens = count_tokens(sent)
        if current_tokens + tokens > max_tokens:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sent
            current_tokens = tokens
        else:
            current_chunk += " " + sent
            current_tokens += tokens

    if current_chunk.strip():
        chunks.append(current_chunk.strip())

    return chunks

def extract_pdf_chunks(path, max_tokens=512):
    doc = fitz.open(path)
    all_chunks = []

    for i, page in enumerate(doc):
        text = page.get_text().strip()
        if not text:
            continue

        sentences = sent_tokenize(text)
        chunks = chunk_sentences(sentences, max_tokens)

        for chunk in chunks:
            all_chunks.append(
                LangChainDocument(
                    page_content=chunk,
                    metadata={"source": path, "page": i + 1}
                )
            )

    return all_chunks
